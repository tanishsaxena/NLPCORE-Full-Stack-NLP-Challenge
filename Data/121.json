{"title": "Analysis of variance", "authors": ["Talgalili"], "time_published": "2020-01-26T11:55:14Z", "sections": [{"title": "", "paragraphs": "{{Use dmy dates|date=June 2013}}\n\n'''Analysis of variance''' ('''ANOVA''') is a collection of [[statistical model]]s and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a [[sample (statistics)|sample]]. ANOVA was developed by [[statistician]] and [[evolutionary biology|evolutionary biologist]] [[Ronald Fisher]]. The ANOVA is based on the [[law of total variance]], where the observed [[variance]] in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a [[statistical test]] of whether two or more population [[mean]]s are equal, and therefore generalizes the [[Student's t-test#Independent two-sample t-test|''t''-test]] beyond two means.\n{{TOC limit}}\n\n", "attributes": [{"name": "Use dmy dates", "values": [{"name": "date", "value": "June 2013"}]}], "id": 0}, {"title": "History", "paragraphs": "While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler.<ref>Stigler (1986)</ref>  These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. [[Pierre-Simon Laplace|Laplace]] was performing hypothesis testing in the 1770s.<ref>Stigler (1986, p 134)</ref> The development of least-squares methods by Laplace and [[Carl Friedrich Gauss|Gauss]] circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares.<ref>Stigler (1986, p 153)</ref> By 1827, Laplace was using [[least squares]] methods to address ANOVA problems regarding measurements of atmospheric tides.<ref>Stigler (1986, pp&nbsp;154\u2013155)</ref> Before 1800, astronomers had isolated observational errors resulting \nfrom reaction times (the \"[[personal equation]]\") and had developed methods of reducing the errors.<ref>Stigler (1986, pp&nbsp;240\u2013242)</ref>  The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology <ref>Stigler (1986, \nChapter 7 \u2013 Psychophysics as a Counterpoint)</ref> which developed strong (full factorial) experimental methods to which randomization and blinding were soon added.<ref>Stigler (1986, p 253)</ref>  An eloquent non-mathematical explanation of the additive effects model was\navailable in 1885.<ref>Stigler (1986, pp&nbsp;314\u2013315)</ref>\n\n[[Ronald Fisher]] introduced the term [[variance]] and proposed its formal analysis in a 1918 article ''[[The Correlation Between Relatives on the Supposition of Mendelian Inheritance]]''.<ref>''The Correlation Between Relatives on the Supposition of Mendelian Inheritance''. Ronald A. Fisher. ''Philosophical Transactions of the Royal Society of Edinburgh''. 1918. (volume 52, pages 399\u2013433)</ref>  His first application of the analysis of variance was published in 1921.<ref>On the \"Probable Error\" of a Coefficient of Correlation Deduced from a Small Sample. Ronald A. Fisher. Metron, 1: 3\u201332 (1921)</ref> Analysis of variance became widely known after being included in Fisher's 1925 book ''[[Statistical Methods for Research Workers]]''.\n\nRandomization models were developed by several researchers.  The first was published in Polish by [[Jerzy Neyman]] in 1923.<ref>Scheff\u00e9 (1959, p 291, \"Randomization models were first formulated by Neyman (1923) for the completely randomized design, by Neyman (1935) for randomized blocks, by Welch (1937) and Pitman (1937) for the Latin square under a certain null hypothesis, and by Kempthorne (1952, 1955) and Wilk (1955) for many other designs.\")</ref>\n\nOne of the attributes of ANOVA that ensured its early popularity was computational elegance.  The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations.  In the era of mechanical calculators this simplicity was critical.  The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts.\n\n", "attributes": [], "id": 1}, {"title": "Example", "paragraphs": "[[File:Anova, no fit..png|thumb|No fit.]][[File:ANOVA fair fit.jpg|thumb|Fair fit]][[File:ANOVA very good fit.jpg|thumb|Very good fit]]The analysis of variance can be used as an exploratory tool to explain observations.  A dog show provides an example.  A dog show is not a random sampling of the breed: it is typically limited to dogs that are adult, pure-bred, and exemplary.  A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations.  Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. One way to do that is to ''explain'' the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that (a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and (b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).\nIn the illustrations to the right, groups are identified as ''X''<sub>1</sub>, ''X''<sub>2</sub>, etc. In the first illustration, the dogs are divided according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (e.g., group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.). Since the distributions of dog weight within each of the groups (shown in blue) has a relatively large variance, and since the means are very similar across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in doesn't allow us to predict its weight much better than simply knowing the dog is in a dog show. Thus, this grouping fails to explain the variation in the overall distribution (yellow-orange).\n\nAn attempt to explain the weight distribution by grouping dogs as ''pet vs working breed'' and ''less athletic vs more athletic'' would probably be somewhat more successful (fair fit).  The heaviest show dogs are likely to be big, strong, working breeds, while breeds kept as pets tend to be smaller and thus lighter.  As shown by the second illustration, the distributions have variances that are considerably smaller than in the first case, and the means are more distinguishable. However, the significant overlap of distributions, for example, means that we cannot distinguish ''X''<sub>1</sub> and ''X''<sub>2</sub> reliably. Grouping dogs according to a coin flip might produce distributions that look similar.\n\nAn attempt to explain weight by breed is likely to produce a very good fit.  All Chihuahuas are light and all St Bernards are heavy.  The difference in weights between Setters and Pointers does not justify separate breeds.  The analysis of variance provides the formal tools to justify these intuitive judgments.  A common use of the method is the analysis of experimental data or the development of models.  The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.\n\n", "attributes": [], "id": 2}, {"title": "Background and terminology", "paragraphs": "ANOVA is a form of [[statistical hypothesis testing]] heavily used in the analysis of experimental data.  A test result (calculated from the [[null hypothesis]] and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, ''assuming the truth of the null hypothesis''. A statistically significant result, when a probability ([[p-value|''p''-value]]) is less than a pre-specified threshold (significance level), justifies the rejection of the [[null hypothesis]], but only if the a priori probability of the null hypothesis is not high.\n\nIn the typical application of ANOVA, the null hypothesis is that all groups are random samples from the same population. For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none). Rejecting the null hypothesis is taken to mean that the differences in observed effects between treatment groups are unlikely to be due to random chance.\n\nBy construction, hypothesis testing limits the rate of [[Type I errors]] (false positives) to a significance level.  Experimenters also wish to limit [[Type II errors]] (false negatives).  \nThe rate of Type II errors depends largely on sample size (the rate is larger for smaller samples), significance \nlevel (when the standard of proof is high, the chances of overlooking \na discovery are also high) and [[effect size]] (a smaller effect size is more prone to Type II error).\n\nThe terminology of ANOVA is largely from the statistical \n[[design of experiments]].  The experimenter adjusts factors and \nmeasures responses in an attempt to determine an effect.  Factors are \nassigned to experimental units by a combination of randomization and \n[[Randomized block design|blocking]] to ensure the validity of the results.  [[Blind experiment|Blinding]] keeps the\nweighing impartial.  Responses show a variability that is partially \nthe result of the effect and is partially random error.\n\nANOVA is the synthesis of several ideas and it is used for multiple \npurposes.  As a consequence, it is difficult to define concisely or precisely.\n\n\"Classical\" ANOVA for balanced data does three things at once:\n{{ordered list|start=1\n| As [[exploratory data analysis]], an ANOVA employs an additive data decomposition, and its sums of squares indicate the variance of each component of the decomposition (or, equivalently, each set of terms of a linear model).\n| Comparisons of mean squares, along with an [[F-test|''F''-test]]&nbsp;... allow testing of a nested sequence of models.\n| Closely related to the ANOVA is a linear model fit with coefficient estimates and standard errors.<ref>Gelman (2005, p 2)</ref>\n}}\nIn short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.\n\nAdditionally:\n{{ordered list|start=4\n| It is computationally elegant and relatively robust against violations of its assumptions.\n| ANOVA provides strong (multiple sample comparison) statistical analysis.\n| It has been adapted to the analysis of a variety of experimental designs.\n}}\nAs a result:\nANOVA \"has long enjoyed the status of being the most used (some would \nsay abused) statistical technique in psychological research.\"<ref>\nHowell (2002, p 320)</ref>\nANOVA \"is probably the most useful technique in the field of \nstatistical inference.\"<ref>Montgomery (2001, p 63)</ref>\n\nANOVA is difficult to teach, particularly for complex experiments, with [[Restricted randomization|split-plot designs]] being notorious.<ref>Gelman (2005, p 1)</ref>  In some cases the proper \napplication of the method is best determined by problem pattern recognition \nfollowed by the consultation of a classic authoritative test.<ref>\nGelman (2005, p 5)</ref>\n\n===Design-of-experiments terms===\n(Condensed from the \"NIST Engineering Statistics Handbook\": Section 5.7. A \nGlossary of DOE Terminology.)<ref>{{cite web\n | title = Section 5.7. A Glossary of DOE Terminology\n | work = NIST Engineering Statistics handbook\n | publisher = NIST\n | url = http://www.itl.nist.gov/div898/handbook/pri/section7/pri7.htm\n | accessdate =  5 April 2012}}</ref>\n\n; Balanced design: An experimental design where all cells (i.e. treatment combinations) have the same number of observations.\n; Blocking: A schedule for conducting treatment combinations in an experimental study such that any effects on the experimental results due to a known change in raw materials, operators, machines, etc., become concentrated in the levels of the blocking variable. The reason for blocking is to isolate a systematic effect and prevent it from obscuring the main effects. Blocking is achieved by restricting randomization.\n; Design: A set of experimental runs which allows the fit of a particular model and the estimate of effects.\n; DOE: Design of experiments.  An approach to problem solving involving collection of data that will support valid, defensible, and supportable conclusions.<ref>{{cite web\n| title = Section 4.3.1 A Glossary of DOE Terminology\n| work= NIST Engineering Statistics handbook\n| publisher = NIST\n| url= http://www.itl.nist.gov/div898/handbook/pmd/section3/pmd31.htm\n| accessdate = 14 Aug 2012}}</ref>\n; Effect: How changing the settings of a factor changes the response. The effect of a single factor is also called a main effect.\n; Error: Unexplained variation in a collection of observations. DOE's typically require understanding of both random error and lack of fit error.\n; Experimental unit: The entity to which a specific treatment combination is applied.\n; Factors: Process inputs that an investigator manipulates to cause a change in the output.\n; Lack-of-fit error: Error that occurs when the analysis omits one or more important terms or factors from the process model. Including replication in a DOE allows separation of experimental error into its components: lack of fit and random (pure) error.\n; Model: Mathematical relationship which relates changes in a given response to changes in one or more factors.\n; Random error: Error that occurs due to natural variation in the process. Random error is typically assumed to be normally distributed with zero mean and a constant variance. Random error is also called experimental error.\n; Randomization: A schedule for allocating treatment material and for conducting treatment combinations in a DOE such that the conditions in one run neither depend on the conditions of the previous run nor predict the conditions in the subsequent runs.<ref group=\"nb\">Randomization is a term used in multiple ways in this\nmaterial.  \"Randomization has three roles in applications: as a device \nfor eliminating biases, for example from unobserved explanatory \nvariables and selection effects; as a basis for estimating standard \nerrors; and as a foundation for formally exact significance tests.\"  \nCox (2006, page 192)  Hinkelmann and Kempthorne use randomization \nboth in experimental design and for statistical analysis.</ref>\n; Replication: Performing the same treatment combination more than once. Including replication allows an estimate of the random error independent of any lack of fit error.\n; Responses: The output(s) of a process. Sometimes called dependent variable(s).\n; Treatment: A treatment is a specific combination of factor levels whose effect is to be compared with other treatments.\n\n", "attributes": [{"name": "ordered list", "values": [{"name": "start", "value": "1"}, {"name": "1", "value": "As [[exploratory data analysis]], an ANOVA employs an additive data decomposition, and its sums of squares indicate the variance of each component of the decomposition (or, equivalently, each set of terms of a linear model)."}, {"name": "2", "value": "Comparisons of mean squares, along with an [[F-test|''F''-test]]&nbsp;... allow testing of a nested sequence of models."}, {"name": "3", "value": "Closely related to the ANOVA is a linear model fit with coefficient estimates and standard errors.<ref>Gelman (2005, p 2)</ref>"}]}, {"name": "ordered list", "values": [{"name": "start", "value": "4"}, {"name": "1", "value": "It is computationally elegant and relatively robust against violations of its assumptions."}, {"name": "2", "value": "ANOVA provides strong (multiple sample comparison) statistical analysis."}, {"name": "3", "value": "It has been adapted to the analysis of a variety of experimental designs."}]}, {"name": "cite web\n ", "values": [{"name": "title", "value": "Section 5.7. A Glossary of DOE Terminology"}, {"name": "work", "value": "NIST Engineering Statistics handbook"}, {"name": "publisher", "value": "NIST"}, {"name": "url", "value": "http://www.itl.nist.gov/div898/handbook/pri/section7/pri7.htm"}, {"name": "accessdate", "value": "5 April 2012"}]}, {"name": "cite web\n", "values": [{"name": "title", "value": "Section 4.3.1 A Glossary of DOE Terminology"}, {"name": "work", "value": "NIST Engineering Statistics handbook"}, {"name": "publisher", "value": "NIST"}, {"name": "url", "value": "http://www.itl.nist.gov/div898/handbook/pmd/section3/pmd31.htm"}, {"name": "accessdate", "value": "14 Aug 2012"}]}], "id": 3}, {"title": "Design-of-experiments terms", "paragraphs": "(Condensed from the \"NIST Engineering Statistics Handbook\": Section 5.7. A \nGlossary of DOE Terminology.)<ref>{{cite web\n | title = Section 5.7. A Glossary of DOE Terminology\n | work = NIST Engineering Statistics handbook\n | publisher = NIST\n | url = http://www.itl.nist.gov/div898/handbook/pri/section7/pri7.htm\n | accessdate =  5 April 2012}}</ref>\n\n; Balanced design: An experimental design where all cells (i.e. treatment combinations) have the same number of observations.\n; Blocking: A schedule for conducting treatment combinations in an experimental study such that any effects on the experimental results due to a known change in raw materials, operators, machines, etc., become concentrated in the levels of the blocking variable. The reason for blocking is to isolate a systematic effect and prevent it from obscuring the main effects. Blocking is achieved by restricting randomization.\n; Design: A set of experimental runs which allows the fit of a particular model and the estimate of effects.\n; DOE: Design of experiments.  An approach to problem solving involving collection of data that will support valid, defensible, and supportable conclusions.<ref>{{cite web\n| title = Section 4.3.1 A Glossary of DOE Terminology\n| work= NIST Engineering Statistics handbook\n| publisher = NIST\n| url= http://www.itl.nist.gov/div898/handbook/pmd/section3/pmd31.htm\n| accessdate = 14 Aug 2012}}</ref>\n; Effect: How changing the settings of a factor changes the response. The effect of a single factor is also called a main effect.\n; Error: Unexplained variation in a collection of observations. DOE's typically require understanding of both random error and lack of fit error.\n; Experimental unit: The entity to which a specific treatment combination is applied.\n; Factors: Process inputs that an investigator manipulates to cause a change in the output.\n; Lack-of-fit error: Error that occurs when the analysis omits one or more important terms or factors from the process model. Including replication in a DOE allows separation of experimental error into its components: lack of fit and random (pure) error.\n; Model: Mathematical relationship which relates changes in a given response to changes in one or more factors.\n; Random error: Error that occurs due to natural variation in the process. Random error is typically assumed to be normally distributed with zero mean and a constant variance. Random error is also called experimental error.\n; Randomization: A schedule for allocating treatment material and for conducting treatment combinations in a DOE such that the conditions in one run neither depend on the conditions of the previous run nor predict the conditions in the subsequent runs.<ref group=\"nb\">Randomization is a term used in multiple ways in this\nmaterial.  \"Randomization has three roles in applications: as a device \nfor eliminating biases, for example from unobserved explanatory \nvariables and selection effects; as a basis for estimating standard \nerrors; and as a foundation for formally exact significance tests.\"  \nCox (2006, page 192)  Hinkelmann and Kempthorne use randomization \nboth in experimental design and for statistical analysis.</ref>\n; Replication: Performing the same treatment combination more than once. Including replication allows an estimate of the random error independent of any lack of fit error.\n; Responses: The output(s) of a process. Sometimes called dependent variable(s).\n; Treatment: A treatment is a specific combination of factor levels whose effect is to be compared with other treatments.\n\n", "attributes": [{"name": "cite web\n ", "values": [{"name": "title", "value": "Section 5.7. A Glossary of DOE Terminology"}, {"name": "work", "value": "NIST Engineering Statistics handbook"}, {"name": "publisher", "value": "NIST"}, {"name": "url", "value": "http://www.itl.nist.gov/div898/handbook/pri/section7/pri7.htm"}, {"name": "accessdate", "value": "5 April 2012"}]}, {"name": "cite web\n", "values": [{"name": "title", "value": "Section 4.3.1 A Glossary of DOE Terminology"}, {"name": "work", "value": "NIST Engineering Statistics handbook"}, {"name": "publisher", "value": "NIST"}, {"name": "url", "value": "http://www.itl.nist.gov/div898/handbook/pmd/section3/pmd31.htm"}, {"name": "accessdate", "value": "14 Aug 2012"}]}], "id": 4}, {"title": " Classes of models ", "paragraphs": "There are three classes of models used in the analysis of variance, and these are outlined here.\n\n===Fixed-effects models===\n{{Main|Fixed effects model}}\nThe fixed-effects model (class I) of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the [[response variable]] values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.\n\n===Random-effects models===\n{{Main|Random effects model}}\nRandom-effects model (class II) is used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are [[random variable]]s, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.<ref>Montgomery (2001, Chapter 12: Experiments with random factors)</ref>\n\n===Mixed-effects models===\n{{Main|Mixed model}}\nA mixed-effects model (class III) contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.\n\nExample:\nTeaching experiments could be performed by a college or university department \nto find a good introductory textbook, with each text considered a \ntreatment.  The fixed-effects model would compare a list of candidate \ntexts.  The random-effects model would determine whether important \ndifferences exist among a list of randomly selected texts.  The \nmixed-effects model would compare the (fixed) incumbent texts to \nrandomly selected alternatives.\n\nDefining fixed and random effects has proven elusive, with competing \ndefinitions arguably leading toward a linguistic quagmire.<ref>\nGelman (2005, pp. 20\u201321)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Fixed effects model"}]}, {"name": "Main", "values": [{"name": "1", "value": "Random effects model"}]}, {"name": "Main", "values": [{"name": "1", "value": "Mixed model"}]}], "id": 5}, {"title": "Fixed-effects models", "paragraphs": "{{Main|Fixed effects model}}\nThe fixed-effects model (class I) of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the [[response variable]] values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Fixed effects model"}]}], "id": 6}, {"title": "Random-effects models", "paragraphs": "{{Main|Random effects model}}\nRandom-effects model (class II) is used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are [[random variable]]s, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.<ref>Montgomery (2001, Chapter 12: Experiments with random factors)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Random effects model"}]}], "id": 7}, {"title": "Mixed-effects models", "paragraphs": "{{Main|Mixed model}}\nA mixed-effects model (class III) contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.\n\nExample:\nTeaching experiments could be performed by a college or university department \nto find a good introductory textbook, with each text considered a \ntreatment.  The fixed-effects model would compare a list of candidate \ntexts.  The random-effects model would determine whether important \ndifferences exist among a list of randomly selected texts.  The \nmixed-effects model would compare the (fixed) incumbent texts to \nrandomly selected alternatives.\n\nDefining fixed and random effects has proven elusive, with competing \ndefinitions arguably leading toward a linguistic quagmire.<ref>\nGelman (2005, pp. 20\u201321)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Mixed model"}]}], "id": 8}, {"title": "Assumptions", "paragraphs": "The analysis of variance has been studied from several approaches, the most common of which uses a [[linear model]] that relates the response to the treatments and blocks. Note that the model is linear in parameters but may be nonlinear across factor levels. Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.\n\n===Textbook analysis using a normal distribution===\nThe analysis of variance can be presented in terms of a [[linear model]], which makes the following assumptions about the [[probability distribution]] of the responses:<ref>{{cite book |title = Statistical Methods\n| last1 = Snedecor | first1 = George W. \n| last2 = Cochran | first2 = William G.\n| year = 1967 | edition = 6th | page = 321\n}}</ref><ref>Cochran & Cox (1992, p 48)</ref><ref>Howell (2002, p 323)</ref><ref>\n{{cite book | last1 = Anderson | first1 = David R.\n| last2 = Sweeney | first2 = Dennis J.\n| last3 = Williams | first3 = Thomas A.\n| title = Statistics for business and economics \n| publisher = West Pub. Co | location = Minneapolis/St. Paul \n| year = 1996 | edition = 6th| isbn = 978-0-314-06378-6 | pages = 452\u2013453}}\n</ref>\n* [[Statistical independence|Independence]] of observations \u2013 this is an assumption of the model that simplifies the statistical analysis.\n* [[normal distribution|Normality]] \u2013 the distributions of the [[Residual (statistics)|residuals]] are [[Normal distribution|normal]].\n* Equality (or \"homogeneity\") of variances, called [[homoscedasticity]] \u2014 the variance of data in groups should be the same.\n\nThe separate assumptions of the textbook model imply that the [[errors and residuals in statistics|errors]] are independently, identically, and normally distributed for fixed effects models, that is, that the errors (<math>\\varepsilon</math>) are independent and\n\n:<math>\\varepsilon \\thicksim N(0, \\sigma^2).\\,</math>\n\n===Randomization-based analysis===\n{{See also|Random assignment|Randomization test}}\nIn a [[Randomized controlled trial|randomized controlled experiment]], the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of [[Charles Sanders Peirce|C. S. Peirce]] and [[Ronald Fisher]]. This design-based analysis was discussed and developed by [[Francis J. Anscombe]] at [[Rothamsted Experimental Station]] and by [[Oscar Kempthorne]] at [[Iowa State University]].<ref>Anscombe (1948)</ref> Kempthorne and his students make an assumption of ''unit treatment additivity'', which is discussed in the books of Kempthorne and [[David R. Cox]].{{Citation needed|date=August 2011}}\n\n====Unit-treatment additivity====\nIn its simplest form, the assumption of unit-treatment additivity<ref group=\"nb\">Unit-treatment additivity is simply termed additivity in most texts. Hinkelmann and Kempthorne add adjectives and distinguish between additivity in the strict and broad senses. This allows a detailed consideration of multiple error sources (treatment, state, selection, measurement and sampling) on page 161.</ref> states that the observed response <math>y_{i,j}</math> from experimental unit <math>i</math> when receiving treatment <math>j</math> can be written as the sum of the unit's response <math>y_i</math> and the treatment-effect <math> t_j</math>, that is <ref>Kempthorne (1979, p 30)</ref><ref name=\"Cox\">Cox (1958, Chapter 2: Some Key Assumptions)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Throughout.  Introduced in Section 2.3.3: Principles of experimental design; The linear model; Outline of a model)</ref>\n: <math>y_{i,j}=y_i+t_j.</math>\nThe assumption of unit-treatment additivity implies that, for every treatment <math>j</math>, the <math>j</math>th treatment has  exactly the same effect <math>t_j</math> on every experiment unit.\n\nThe assumption of unit treatment additivity  usually cannot be directly [[falsificationism|falsified]], according to Cox and Kempthorne. However, many ''consequences'' of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity ''implies'' that the variance is constant for all treatments. Therefore, by [[contraposition]], a necessary condition for unit-treatment additivity is that the variance is constant.\n\nThe use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population [[survey sampling]].\n\n====Derived linear model====\nKempthorne uses the randomization-distribution and the assumption of ''unit treatment additivity'' to produce a ''derived linear model'', very similar to the textbook model discussed previously.<ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.3: \nCompletely Randomized Design; Derived Linear Model)</ref>  The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies.<ref name=\"HinkelmannKempthorne\">Hinkelmann and Kempthorne (2008, Volume 1, Section 6.6: Completely randomized design; Approximating the randomization test)</ref> However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations.<ref>Bailey (2008, Chapter 2.14 \"A More General Model\" in Bailey, pp.&nbsp;38\u201340)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Chapter 7: Comparison of Treatments)</ref> In the randomization-based analysis, there is ''no assumption'' of a ''normal'' distribution and certainly ''no assumption'' of ''independence''. On the contrary, ''the observations are dependent''!\n\nThe randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time.  Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.\n\n====Statistical models for observational data====\nHowever, when applied to data from non-randomized experiments or [[observational study|observational studies]], model-based analysis lacks the warrant of randomization.<ref>\nKempthorne (1979, pp 125\u2013126, \n\"The experimenter must decide which of the various causes that he \nfeels will produce variations in his results must be controlled \nexperimentally.  Those causes that he does not control experimentally, \nbecause he is not cognizant of them, he must control by the device of \nrandomization.\"  \"[O]nly when the treatments in the experiment are \napplied by the experimenter using the full randomization procedure is \nthe chain of inductive inference sound.  It is ''only'' under these \ncircumstances that the experimenter can attribute whatever effects he \nobserves to the treatment and the treatment only.  Under these \ncircumstances his conclusions are reliable in the statistical sense.\")  \n</ref> For observational data, the derivation of confidence intervals must use ''subjective'' models, as emphasized by [[Ronald Fisher]] and his followers. In practice, the estimates of treatment-effects from observational studies  generally are often inconsistent.  In practice, \"statistical models\" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.<ref>Freedman {{full citation needed|date=November 2012}}</ref>\n\n===Summary of assumptions===\nThe normal-model based ANOVA analysis assumes the independence, normality and \nhomogeneity of variances of the residuals. The \nrandomization-based analysis assumes only the homogeneity of the \nvariances of the residuals (as a consequence of unit-treatment \nadditivity) and uses the randomization procedure of the experiment. \nBoth these analyses require [[homoscedasticity]], as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.\n\nHowever, studies of processes that \nchange variances rather than means (called dispersion effects) have \nbeen successfully conducted using ANOVA.<ref>Montgomery \n(2001, Section 3.8: Discovering dispersion effects)</ref>  There are\n''no'' necessary assumptions for ANOVA in its full generality, but the\n''F''-test used for ANOVA hypothesis testing has assumptions and practical \nlimitations which are of continuing interest.\n\nProblems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. \nThe property of unit-treatment additivity is not invariant under a \"change of scale\", so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance.<ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.10: Completely randomized design; Transformations)</ref> Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.<ref name=\"Cox\" /><ref>Bailey (2008)</ref>\nAccording to Cauchy's [[functional equation]] theorem, the [[logarithm]] is the only continuous transformation that transforms real multiplication to addition.{{citation needed|date=October 2013}}\n\n", "attributes": [{"name": "cite book ", "values": [{"name": "title", "value": "Statistical Methods"}, {"name": "last1", "value": "Snedecor"}, {"name": "first1", "value": "George W."}, {"name": "last2", "value": "Cochran"}, {"name": "first2", "value": "William G."}, {"name": "year", "value": "1967"}, {"name": "edition", "value": "6th"}, {"name": "page", "value": "321"}]}, {"name": "cite book ", "values": [{"name": "last1", "value": "Anderson"}, {"name": "first1", "value": "David R."}, {"name": "last2", "value": "Sweeney"}, {"name": "first2", "value": "Dennis J."}, {"name": "last3", "value": "Williams"}, {"name": "first3", "value": "Thomas A."}, {"name": "title", "value": "Statistics for business and economics"}, {"name": "publisher", "value": "West Pub. Co"}, {"name": "location", "value": "Minneapolis/St. Paul"}, {"name": "year", "value": "1996"}, {"name": "edition", "value": "6th"}, {"name": "isbn", "value": "978-0-314-06378-6"}, {"name": "pages", "value": "452\u2013453"}]}, {"name": "See also", "values": [{"name": "1", "value": "Random assignment"}, {"name": "2", "value": "Randomization test"}]}, {"name": "Citation needed", "values": [{"name": "date", "value": "August 2011"}]}, {"name": "full citation needed", "values": [{"name": "date", "value": "November 2012"}]}, {"name": "citation needed", "values": [{"name": "date", "value": "October 2013"}]}], "id": 9}, {"title": "Textbook analysis using a normal distribution", "paragraphs": "The analysis of variance can be presented in terms of a [[linear model]], which makes the following assumptions about the [[probability distribution]] of the responses:<ref>{{cite book |title = Statistical Methods\n| last1 = Snedecor | first1 = George W. \n| last2 = Cochran | first2 = William G.\n| year = 1967 | edition = 6th | page = 321\n}}</ref><ref>Cochran & Cox (1992, p 48)</ref><ref>Howell (2002, p 323)</ref><ref>\n{{cite book | last1 = Anderson | first1 = David R.\n| last2 = Sweeney | first2 = Dennis J.\n| last3 = Williams | first3 = Thomas A.\n| title = Statistics for business and economics \n| publisher = West Pub. Co | location = Minneapolis/St. Paul \n| year = 1996 | edition = 6th| isbn = 978-0-314-06378-6 | pages = 452\u2013453}}\n</ref>\n* [[Statistical independence|Independence]] of observations \u2013 this is an assumption of the model that simplifies the statistical analysis.\n* [[normal distribution|Normality]] \u2013 the distributions of the [[Residual (statistics)|residuals]] are [[Normal distribution|normal]].\n* Equality (or \"homogeneity\") of variances, called [[homoscedasticity]] \u2014 the variance of data in groups should be the same.\n\nThe separate assumptions of the textbook model imply that the [[errors and residuals in statistics|errors]] are independently, identically, and normally distributed for fixed effects models, that is, that the errors (<math>\\varepsilon</math>) are independent and\n\n:<math>\\varepsilon \\thicksim N(0, \\sigma^2).\\,</math>\n\n", "attributes": [{"name": "cite book ", "values": [{"name": "title", "value": "Statistical Methods"}, {"name": "last1", "value": "Snedecor"}, {"name": "first1", "value": "George W."}, {"name": "last2", "value": "Cochran"}, {"name": "first2", "value": "William G."}, {"name": "year", "value": "1967"}, {"name": "edition", "value": "6th"}, {"name": "page", "value": "321"}]}, {"name": "cite book ", "values": [{"name": "last1", "value": "Anderson"}, {"name": "first1", "value": "David R."}, {"name": "last2", "value": "Sweeney"}, {"name": "first2", "value": "Dennis J."}, {"name": "last3", "value": "Williams"}, {"name": "first3", "value": "Thomas A."}, {"name": "title", "value": "Statistics for business and economics"}, {"name": "publisher", "value": "West Pub. Co"}, {"name": "location", "value": "Minneapolis/St. Paul"}, {"name": "year", "value": "1996"}, {"name": "edition", "value": "6th"}, {"name": "isbn", "value": "978-0-314-06378-6"}, {"name": "pages", "value": "452\u2013453"}]}], "id": 10}, {"title": "Randomization-based analysis", "paragraphs": "{{See also|Random assignment|Randomization test}}\nIn a [[Randomized controlled trial|randomized controlled experiment]], the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of [[Charles Sanders Peirce|C. S. Peirce]] and [[Ronald Fisher]]. This design-based analysis was discussed and developed by [[Francis J. Anscombe]] at [[Rothamsted Experimental Station]] and by [[Oscar Kempthorne]] at [[Iowa State University]].<ref>Anscombe (1948)</ref> Kempthorne and his students make an assumption of ''unit treatment additivity'', which is discussed in the books of Kempthorne and [[David R. Cox]].{{Citation needed|date=August 2011}}\n\n====Unit-treatment additivity====\nIn its simplest form, the assumption of unit-treatment additivity<ref group=\"nb\">Unit-treatment additivity is simply termed additivity in most texts. Hinkelmann and Kempthorne add adjectives and distinguish between additivity in the strict and broad senses. This allows a detailed consideration of multiple error sources (treatment, state, selection, measurement and sampling) on page 161.</ref> states that the observed response <math>y_{i,j}</math> from experimental unit <math>i</math> when receiving treatment <math>j</math> can be written as the sum of the unit's response <math>y_i</math> and the treatment-effect <math> t_j</math>, that is <ref>Kempthorne (1979, p 30)</ref><ref name=\"Cox\">Cox (1958, Chapter 2: Some Key Assumptions)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Throughout.  Introduced in Section 2.3.3: Principles of experimental design; The linear model; Outline of a model)</ref>\n: <math>y_{i,j}=y_i+t_j.</math>\nThe assumption of unit-treatment additivity implies that, for every treatment <math>j</math>, the <math>j</math>th treatment has  exactly the same effect <math>t_j</math> on every experiment unit.\n\nThe assumption of unit treatment additivity  usually cannot be directly [[falsificationism|falsified]], according to Cox and Kempthorne. However, many ''consequences'' of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity ''implies'' that the variance is constant for all treatments. Therefore, by [[contraposition]], a necessary condition for unit-treatment additivity is that the variance is constant.\n\nThe use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population [[survey sampling]].\n\n====Derived linear model====\nKempthorne uses the randomization-distribution and the assumption of ''unit treatment additivity'' to produce a ''derived linear model'', very similar to the textbook model discussed previously.<ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.3: \nCompletely Randomized Design; Derived Linear Model)</ref>  The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies.<ref name=\"HinkelmannKempthorne\">Hinkelmann and Kempthorne (2008, Volume 1, Section 6.6: Completely randomized design; Approximating the randomization test)</ref> However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations.<ref>Bailey (2008, Chapter 2.14 \"A More General Model\" in Bailey, pp.&nbsp;38\u201340)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Chapter 7: Comparison of Treatments)</ref> In the randomization-based analysis, there is ''no assumption'' of a ''normal'' distribution and certainly ''no assumption'' of ''independence''. On the contrary, ''the observations are dependent''!\n\nThe randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time.  Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.\n\n====Statistical models for observational data====\nHowever, when applied to data from non-randomized experiments or [[observational study|observational studies]], model-based analysis lacks the warrant of randomization.<ref>\nKempthorne (1979, pp 125\u2013126, \n\"The experimenter must decide which of the various causes that he \nfeels will produce variations in his results must be controlled \nexperimentally.  Those causes that he does not control experimentally, \nbecause he is not cognizant of them, he must control by the device of \nrandomization.\"  \"[O]nly when the treatments in the experiment are \napplied by the experimenter using the full randomization procedure is \nthe chain of inductive inference sound.  It is ''only'' under these \ncircumstances that the experimenter can attribute whatever effects he \nobserves to the treatment and the treatment only.  Under these \ncircumstances his conclusions are reliable in the statistical sense.\")  \n</ref> For observational data, the derivation of confidence intervals must use ''subjective'' models, as emphasized by [[Ronald Fisher]] and his followers. In practice, the estimates of treatment-effects from observational studies  generally are often inconsistent.  In practice, \"statistical models\" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.<ref>Freedman {{full citation needed|date=November 2012}}</ref>\n\n", "attributes": [{"name": "See also", "values": [{"name": "1", "value": "Random assignment"}, {"name": "2", "value": "Randomization test"}]}, {"name": "Citation needed", "values": [{"name": "date", "value": "August 2011"}]}, {"name": "full citation needed", "values": [{"name": "date", "value": "November 2012"}]}], "id": 11}, {"title": "Unit-treatment additivity", "paragraphs": "In its simplest form, the assumption of unit-treatment additivity<ref group=\"nb\">Unit-treatment additivity is simply termed additivity in most texts. Hinkelmann and Kempthorne add adjectives and distinguish between additivity in the strict and broad senses. This allows a detailed consideration of multiple error sources (treatment, state, selection, measurement and sampling) on page 161.</ref> states that the observed response <math>y_{i,j}</math> from experimental unit <math>i</math> when receiving treatment <math>j</math> can be written as the sum of the unit's response <math>y_i</math> and the treatment-effect <math> t_j</math>, that is <ref>Kempthorne (1979, p 30)</ref><ref name=\"Cox\">Cox (1958, Chapter 2: Some Key Assumptions)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Throughout.  Introduced in Section 2.3.3: Principles of experimental design; The linear model; Outline of a model)</ref>\n: <math>y_{i,j}=y_i+t_j.</math>\nThe assumption of unit-treatment additivity implies that, for every treatment <math>j</math>, the <math>j</math>th treatment has  exactly the same effect <math>t_j</math> on every experiment unit.\n\nThe assumption of unit treatment additivity  usually cannot be directly [[falsificationism|falsified]], according to Cox and Kempthorne. However, many ''consequences'' of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity ''implies'' that the variance is constant for all treatments. Therefore, by [[contraposition]], a necessary condition for unit-treatment additivity is that the variance is constant.\n\nThe use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population [[survey sampling]].\n\n", "attributes": [], "id": 12}, {"title": "Derived linear model", "paragraphs": "Kempthorne uses the randomization-distribution and the assumption of ''unit treatment additivity'' to produce a ''derived linear model'', very similar to the textbook model discussed previously.<ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.3: \nCompletely Randomized Design; Derived Linear Model)</ref>  The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies.<ref name=\"HinkelmannKempthorne\">Hinkelmann and Kempthorne (2008, Volume 1, Section 6.6: Completely randomized design; Approximating the randomization test)</ref> However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations.<ref>Bailey (2008, Chapter 2.14 \"A More General Model\" in Bailey, pp.&nbsp;38\u201340)</ref><ref>Hinkelmann and Kempthorne (2008, Volume 1, Chapter 7: Comparison of Treatments)</ref> In the randomization-based analysis, there is ''no assumption'' of a ''normal'' distribution and certainly ''no assumption'' of ''independence''. On the contrary, ''the observations are dependent''!\n\nThe randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time.  Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.\n\n", "attributes": [], "id": 13}, {"title": "Statistical models for observational data", "paragraphs": "However, when applied to data from non-randomized experiments or [[observational study|observational studies]], model-based analysis lacks the warrant of randomization.<ref>\nKempthorne (1979, pp 125\u2013126, \n\"The experimenter must decide which of the various causes that he \nfeels will produce variations in his results must be controlled \nexperimentally.  Those causes that he does not control experimentally, \nbecause he is not cognizant of them, he must control by the device of \nrandomization.\"  \"[O]nly when the treatments in the experiment are \napplied by the experimenter using the full randomization procedure is \nthe chain of inductive inference sound.  It is ''only'' under these \ncircumstances that the experimenter can attribute whatever effects he \nobserves to the treatment and the treatment only.  Under these \ncircumstances his conclusions are reliable in the statistical sense.\")  \n</ref> For observational data, the derivation of confidence intervals must use ''subjective'' models, as emphasized by [[Ronald Fisher]] and his followers. In practice, the estimates of treatment-effects from observational studies  generally are often inconsistent.  In practice, \"statistical models\" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.<ref>Freedman {{full citation needed|date=November 2012}}</ref>\n\n", "attributes": [{"name": "full citation needed", "values": [{"name": "date", "value": "November 2012"}]}], "id": 14}, {"title": "Summary of assumptions", "paragraphs": "The normal-model based ANOVA analysis assumes the independence, normality and \nhomogeneity of variances of the residuals. The \nrandomization-based analysis assumes only the homogeneity of the \nvariances of the residuals (as a consequence of unit-treatment \nadditivity) and uses the randomization procedure of the experiment. \nBoth these analyses require [[homoscedasticity]], as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.\n\nHowever, studies of processes that \nchange variances rather than means (called dispersion effects) have \nbeen successfully conducted using ANOVA.<ref>Montgomery \n(2001, Section 3.8: Discovering dispersion effects)</ref>  There are\n''no'' necessary assumptions for ANOVA in its full generality, but the\n''F''-test used for ANOVA hypothesis testing has assumptions and practical \nlimitations which are of continuing interest.\n\nProblems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. \nThe property of unit-treatment additivity is not invariant under a \"change of scale\", so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance.<ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.10: Completely randomized design; Transformations)</ref> Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.<ref name=\"Cox\" /><ref>Bailey (2008)</ref>\nAccording to Cauchy's [[functional equation]] theorem, the [[logarithm]] is the only continuous transformation that transforms real multiplication to addition.{{citation needed|date=October 2013}}\n\n", "attributes": [{"name": "citation needed", "values": [{"name": "date", "value": "October 2013"}]}], "id": 15}, {"title": "Characteristics", "paragraphs": "ANOVA is used in the analysis of comparative experiments, those in \nwhich only the difference in outcomes is of interest.  The statistical\nsignificance of the experiment is determined by a ratio of two \nvariances.  This ratio is independent of several possible alterations\nto the experimental observations: Adding a constant to all \nobservations does not alter significance.  Multiplying all \nobservations by a constant does not alter significance.  So ANOVA \nstatistical significance result is independent of constant bias and \nscaling errors as well as the units used in expressing observations.  \nIn the era of mechanical calculation it was common to \nsubtract a constant from all observations (when equivalent to \ndropping leading digits) to simplify data entry.<ref>Montgomery \n(2001, Section 3-3: Experiments with a single factor: The analysis of \nvariance; Analysis of the fixed effects model)</ref><ref>\nCochran & Cox (1992, p 2 example)</ref>  This is an example of data\n[[Coding (social sciences)|coding]].\n\n", "attributes": [], "id": 16}, {"title": "Logic", "paragraphs": "The calculations of ANOVA can be characterized as computing a number\nof means and variances, dividing two variances and comparing the ratio \nto a handbook value to determine statistical significance.  Calculating \na treatment effect is then trivial, \"the effect of any treatment is \nestimated by taking the difference between the mean of the \nobservations which receive the treatment and the general mean\".<ref>\nCochran & Cox (1992, p 49)</ref>\n\n===Partitioning of the sum of squares===\n{{main|Partition of sums of squares}}\nANOVA uses traditional standardized terminology.  The definitional \nequation of sample variance is\n<math>s^2=\\textstyle\\frac{1}{n-1}\\sum(y_i-\\bar{y})^2</math>, where the \ndivisor is called the degrees of freedom (DF), the summation is called \nthe sum of squares (SS), the result is called the mean square (MS) and \nthe squared terms are deviations from the sample mean.  ANOVA \nestimates 3 sample variances: a total variance based on all the \nobservation deviations from the grand mean, an error variance based on \nall the observation deviations from their appropriate \ntreatment means, and a treatment variance.  The treatment variance is\nbased on the deviations of treatment means from the grand mean, the \nresult being multiplied by the number of observations in each \ntreatment to account for the difference between the variance of \nobservations and the variance of means.\n\nThe fundamental technique is a partitioning of the total [[sum of squares (statistics)|sum of squares]] ''SS'' into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels.\n\n:<math>SS_\\text{Total} = SS_\\text{Error} + SS_\\text{Treatments}</math>\n\nThe number of [[Degrees of freedom (statistics)|degrees of freedom]] ''DF'' can be partitioned in a similar way: one of these components (that for error) specifies a [[chi-squared distribution]] which describes the associated sum of squares, while the same is true for \"treatments\" if there is no treatment effect.\n\n:<math>DF_\\text{Total} = DF_\\text{Error} + DF_\\text{Treatments}</math>\n\nSee also [[Lack-of-fit sum of squares]].\n\n===The ''F''-test===\n{{Main|F-test}}\nThe [[F-test|''F''-test]] is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic\n\n:<math>F = \\frac{\\text{variance between treatments}}{\\text{variance within treatments}}</math>\n\n:<math>F = \\frac{MS_\\text{Treatments}}{MS_\\text{Error}} = {{SS_\\text{Treatments} / (I-1)} \\over {SS_\\text{Error} / (n_T-I)}}</math>\nwhere  ''MS'' is mean square, <math>I</math>  = number of treatments and \n<math>n_T</math> = total number of cases\n\nto the [[F-distribution|''F''-distribution]] with <math>I - 1</math>, <math>n_T - I</math>  degrees of freedom. Using the ''F''-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled [[chi-squared distribution]].\n\nThe expected value of F is <math>1 + {n \\sigma^2_\\text{Treatment}} / \n{\\sigma^2_\\text{Error}}</math> (where <math>n</math> is the treatment sample size)\nwhich is 1 for no treatment effect.  As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.\n\nThere are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:\n* The textbook method is to compare the observed value of F with the critical value of F determined from tables. The critical value of F is a function of the degrees of freedom of the numerator and the denominator and the significance level (\u03b1).  If F \u2265 F<sub>Critical</sub>, the null hypothesis is rejected.\n* The computer method calculates the probability (p-value) of a value of F greater than or equal to the observed value.  The null hypothesis is rejected if this probability is less than or equal to the significance level (\u03b1).\nThe ANOVA ''F''-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the [[F-test|''F''-test]]'s ''p''-values closely approximate the [[permutation test]]'s [[p-value]]s: The approximation is particularly close when the design is balanced.<ref name=\"HinkelmannKempthorne\" /><ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.7: Completely randomized design; CRD with unequal numbers of replications)</ref> Such [[permutation test]]s characterize [[uniformly most powerful test|tests with maximum power]] against all [[alternative hypothesis|alternative hypotheses]], as observed by Rosenbaum.<ref group=\"nb\">Rosenbaum (2002, page 40) cites Section 5.7 (Permutation Tests), Theorem 2.3 (actually Theorem 3, page 184) of [[Erich Leo Lehmann|Lehmann]]'s ''Testing Statistical Hypotheses'' (1959).</ref> The ANOVA ''F''-test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.<ref>Moore and McCabe (2003, page 763)</ref><ref group=\"nb\">The ''F''-test for the comparison of variances has a mixed reputation.  It \nis not recommended as a hypothesis test to determine whether two \n''different'' samples have the same variance.  It is recommended for \nANOVA where two estimates of the variance of the ''same'' \nsample are compared.  While the ''F''-test is not generally robust against \ndepartures from normality, it has been found to be robust in the \nspecial case of ANOVA.  Citations from Moore & McCabe (2003): \n\"Analysis of variance uses F statistics, but these are not \nthe same as the F statistic for comparing two population standard \ndeviations.\" (page 554) \"The F test and other procedures for inference \nabout variances are so lacking in robustness as to be of little use in \npractice.\" (page 556)  \"[The ANOVA ''F''-test] is relatively insensitive \nto moderate nonnormality and unequal variances, especially when the \nsample sizes are similar.\" (page 763)  ANOVA assumes homoscedasticity, \nbut it is robust.  The statistical test for homoscedasticity (the \n''F''-test) is not robust.  Moore & McCabe recommend a rule of thumb.</ref>\n\n===Extended logic===\nANOVA consists of separable parts; partitioning sources of variance \nand hypothesis testing can be used individually.  ANOVA is used to \nsupport other statistical tools.  Regression is first used to fit more \ncomplex models to data, then ANOVA is used to compare models with the \nobjective of selecting simple(r) models that adequately describe the \ndata.  \"Such models could be fit without any reference to ANOVA, but \nANOVA tools could then be used to make some sense of the fitted models, \nand to test hypotheses about batches of coefficients.\"<ref name=\"Gelman\">Gelman (2008)</ref>  \n\"[W]e think of the analysis of variance as a way of understanding and structuring \nmultilevel models\u2014not as an alternative to regression but as a tool \nfor summarizing complex high-dimensional inferences&nbsp;...\"<ref name=\"Gelman\" />\n\n", "attributes": [{"name": "main", "values": [{"name": "1", "value": "Partition of sums of squares"}]}, {"name": "Main", "values": [{"name": "1", "value": "F-test"}]}], "id": 17}, {"title": "Partitioning of the sum of squares", "paragraphs": "{{main|Partition of sums of squares}}\nANOVA uses traditional standardized terminology.  The definitional \nequation of sample variance is\n<math>s^2=\\textstyle\\frac{1}{n-1}\\sum(y_i-\\bar{y})^2</math>, where the \ndivisor is called the degrees of freedom (DF), the summation is called \nthe sum of squares (SS), the result is called the mean square (MS) and \nthe squared terms are deviations from the sample mean.  ANOVA \nestimates 3 sample variances: a total variance based on all the \nobservation deviations from the grand mean, an error variance based on \nall the observation deviations from their appropriate \ntreatment means, and a treatment variance.  The treatment variance is\nbased on the deviations of treatment means from the grand mean, the \nresult being multiplied by the number of observations in each \ntreatment to account for the difference between the variance of \nobservations and the variance of means.\n\nThe fundamental technique is a partitioning of the total [[sum of squares (statistics)|sum of squares]] ''SS'' into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels.\n\n:<math>SS_\\text{Total} = SS_\\text{Error} + SS_\\text{Treatments}</math>\n\nThe number of [[Degrees of freedom (statistics)|degrees of freedom]] ''DF'' can be partitioned in a similar way: one of these components (that for error) specifies a [[chi-squared distribution]] which describes the associated sum of squares, while the same is true for \"treatments\" if there is no treatment effect.\n\n:<math>DF_\\text{Total} = DF_\\text{Error} + DF_\\text{Treatments}</math>\n\nSee also [[Lack-of-fit sum of squares]].\n\n", "attributes": [{"name": "main", "values": [{"name": "1", "value": "Partition of sums of squares"}]}], "id": 18}, {"title": "The ''F''-test", "paragraphs": "{{Main|F-test}}\nThe [[F-test|''F''-test]] is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic\n\n:<math>F = \\frac{\\text{variance between treatments}}{\\text{variance within treatments}}</math>\n\n:<math>F = \\frac{MS_\\text{Treatments}}{MS_\\text{Error}} = {{SS_\\text{Treatments} / (I-1)} \\over {SS_\\text{Error} / (n_T-I)}}</math>\nwhere  ''MS'' is mean square, <math>I</math>  = number of treatments and \n<math>n_T</math> = total number of cases\n\nto the [[F-distribution|''F''-distribution]] with <math>I - 1</math>, <math>n_T - I</math>  degrees of freedom. Using the ''F''-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled [[chi-squared distribution]].\n\nThe expected value of F is <math>1 + {n \\sigma^2_\\text{Treatment}} / \n{\\sigma^2_\\text{Error}}</math> (where <math>n</math> is the treatment sample size)\nwhich is 1 for no treatment effect.  As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.\n\nThere are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:\n* The textbook method is to compare the observed value of F with the critical value of F determined from tables. The critical value of F is a function of the degrees of freedom of the numerator and the denominator and the significance level (\u03b1).  If F \u2265 F<sub>Critical</sub>, the null hypothesis is rejected.\n* The computer method calculates the probability (p-value) of a value of F greater than or equal to the observed value.  The null hypothesis is rejected if this probability is less than or equal to the significance level (\u03b1).\nThe ANOVA ''F''-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the [[F-test|''F''-test]]'s ''p''-values closely approximate the [[permutation test]]'s [[p-value]]s: The approximation is particularly close when the design is balanced.<ref name=\"HinkelmannKempthorne\" /><ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 6.7: Completely randomized design; CRD with unequal numbers of replications)</ref> Such [[permutation test]]s characterize [[uniformly most powerful test|tests with maximum power]] against all [[alternative hypothesis|alternative hypotheses]], as observed by Rosenbaum.<ref group=\"nb\">Rosenbaum (2002, page 40) cites Section 5.7 (Permutation Tests), Theorem 2.3 (actually Theorem 3, page 184) of [[Erich Leo Lehmann|Lehmann]]'s ''Testing Statistical Hypotheses'' (1959).</ref> The ANOVA ''F''-test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.<ref>Moore and McCabe (2003, page 763)</ref><ref group=\"nb\">The ''F''-test for the comparison of variances has a mixed reputation.  It \nis not recommended as a hypothesis test to determine whether two \n''different'' samples have the same variance.  It is recommended for \nANOVA where two estimates of the variance of the ''same'' \nsample are compared.  While the ''F''-test is not generally robust against \ndepartures from normality, it has been found to be robust in the \nspecial case of ANOVA.  Citations from Moore & McCabe (2003): \n\"Analysis of variance uses F statistics, but these are not \nthe same as the F statistic for comparing two population standard \ndeviations.\" (page 554) \"The F test and other procedures for inference \nabout variances are so lacking in robustness as to be of little use in \npractice.\" (page 556)  \"[The ANOVA ''F''-test] is relatively insensitive \nto moderate nonnormality and unequal variances, especially when the \nsample sizes are similar.\" (page 763)  ANOVA assumes homoscedasticity, \nbut it is robust.  The statistical test for homoscedasticity (the \n''F''-test) is not robust.  Moore & McCabe recommend a rule of thumb.</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "F-test"}]}], "id": 19}, {"title": "Extended logic", "paragraphs": "ANOVA consists of separable parts; partitioning sources of variance \nand hypothesis testing can be used individually.  ANOVA is used to \nsupport other statistical tools.  Regression is first used to fit more \ncomplex models to data, then ANOVA is used to compare models with the \nobjective of selecting simple(r) models that adequately describe the \ndata.  \"Such models could be fit without any reference to ANOVA, but \nANOVA tools could then be used to make some sense of the fitted models, \nand to test hypotheses about batches of coefficients.\"<ref name=\"Gelman\">Gelman (2008)</ref>  \n\"[W]e think of the analysis of variance as a way of understanding and structuring \nmultilevel models\u2014not as an alternative to regression but as a tool \nfor summarizing complex high-dimensional inferences&nbsp;...\"<ref name=\"Gelman\" />\n\n", "attributes": [], "id": 20}, {"title": "For a single factor", "paragraphs": "{{Main|One-way analysis of variance}}\nThe simplest experiment suitable for ANOVA analysis is the completely \nrandomized experiment with a single factor.  More complex experiments \nwith a single factor involve constraints on randomization and include \ncompletely randomized blocks and Latin squares (and variants: \nGraeco-Latin squares, etc.).  The more complex experiments share many \nof the complexities of multiple factors.  A relatively complete \ndiscussion of the analysis (models, data summaries, ANOVA table) of \nthe completely randomized experiment is \n[[One-way analysis of variance|available]].\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "One-way analysis of variance"}]}], "id": 21}, {"title": "For multiple factors", "paragraphs": "{{Main|Two-way analysis of variance}}\nANOVA generalizes to the study of the effects of multiple factors.  \nWhen the experiment includes observations at all combinations of \nlevels of each factor, it is termed [[Factorial experiment|factorial]].  \nFactorial experiments \nare more efficient than a series of single factor experiments and the \nefficiency grows as the number of factors increases.<ref name=\"Montgomery\">Montgomery \n(2001, Section 5-2: Introduction to factorial designs; The advantages \nof factorials)</ref>  Consequently, factorial designs are heavily used.\n\nThe use of ANOVA to study the effects of multiple factors has a complication.  In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x, y, z) and terms for [[Interaction (statistics)|interactions]] (xy, xz, yz, xyz).  \nAll terms require hypothesis tests.  The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare.<ref>Belle (2008, Section 8.4: High-order interactions occur rarely)</ref>   {{verify source|date=December 2014}}\nThe ability to detect interactions is a major advantage of multiple \nfactor ANOVA.  Testing one factor at a time hides interactions, but \nproduces apparently inconsistent experimental results.<ref name=\"Montgomery\" />\n\nCaution is advised when encountering interactions; Test  \ninteraction terms first and expand the analysis beyond ANOVA if \ninteractions are found.  Texts vary in their recommendations regarding \nthe continuation of the ANOVA procedure after encountering an \ninteraction.  Interactions complicate the interpretation of \nexperimental data.  Neither the calculations of significance nor the \nestimated treatment effects can be taken at face value.  \"A \nsignificant interaction will often mask the significance of main effects.\"<ref>Montgomery (2001, Section 5-1: Introduction to factorial designs; Basic definitions and principles)</ref>  Graphical methods are recommended\nto enhance understanding.  Regression is often useful.  A lengthy discussion of interactions is available in Cox (1958).<ref>Cox (1958, \nChapter 6: Basic ideas about factorial experiments)</ref>  Some interactions can be removed (by transformations) while others cannot.\n\nA variety of techniques are used with multiple factor ANOVA to reduce expense. One technique used in factorial designs is to minimize replication (possibly no replication with support of [[Tukey's test of additivity|analytical trickery]]) and to combine groups when effects are found to be statistically (or practically) insignificant.  An experiment with many insignificant factors may collapse into one with a few factors supported by many replications.<ref>Montgomery (2001, Section 5-3.7: Introduction to factorial designs; The two-factor factorial design; One observation per cell)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Two-way analysis of variance"}]}, {"name": "verify source", "values": [{"name": "date", "value": "December 2014"}]}], "id": 22}, {"title": "Worked numeric examples", "paragraphs": "Numerous fully worked numerical examples are available in standard textbooks and online.  A \n[[One-way analysis of variance#Example|simple case]] uses one-way (a single factor) analysis.  \n<!--  EXAMPLE NOT PRESENT IN LINKED ARTICLE:  A [[Two-way analysis of variance|more complex case]] uses two-way (two-factor) analysis.-->\n\n", "attributes": [], "id": 23}, {"title": "Associated analysis", "paragraphs": "Some analysis is required in support of the ''design'' of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses.  Because experimentation is iterative, the results of one experiment alter plans for following experiments.\n\n===Preparatory analysis===\n\n====The number of experimental units====\n\nIn the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.\n\nEarly experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.\n\nReporting sample size analysis is generally required in psychology. \"Provide information on sample size and the process that led to sample size decisions.\"<ref>Wilkinson (1999, p 596)</ref>  The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.\n\nBesides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting\nthe probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.<ref>Montgomery (2001, Section 3-7: Determining sample size)</ref>\n\n====Power analysis====\n[[Statistical power|Power analysis]] is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.<ref>Howell (2002, Chapter 8: Power)</ref><ref>Howell (2002, Section 11.12: Power (in ANOVA))</ref><ref>Howell (2002, Section 13.7: Power analysis for factorial experiments)</ref><ref>Moore and McCabe (2003, pp 778\u2013780)</ref>\n\n====Effect size====\n{{Main|Effect size}}\nSeveral standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable or the overall standardized difference of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines.  However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately \"meaningful\" units may be preferable for reporting purposes.<ref name=\"Wilkinson\">Wilkinson (1999, p 599)</ref>\n\n===Follow-up analysis===\nIt is always appropriate to carefully consider outliers.  They have a disproportionate impact on statistical conclusions and are often the result of errors.\n\n====Model confirmation====\nIt is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm [[homoscedasticity]] and gross normality.<ref>Montgomery (2001, Section 3-4: Model adequacy checking)</ref>  Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and \nmodeled data values. Trends hint at interactions among factors or among observations.  One rule of thumb: \"If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results \nwill still be approximately correct.\"<ref>Moore and McCabe (2003, p 755, Qualifications to this rule appear in a footnote.)</ref>\n\n====Follow-up tests====\nA statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned ([[A priori and a posteriori|a priori]]) or [[Post-hoc analysis|post hoc]]. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.\n\nOften one of the \"treatments\" is none, so the treatment group can act as a control. [[Dunnett's test]] (a modification of the ''t''-test) tests whether each of the other treatment groups has the same mean as the control.<ref>Montgomery (2001, Section 3-5.8: Experiments with a single factor: The analysis of variance; Practical interpretation of results; Comparing means with a control)</ref>\n\nPost hoc tests such as [[Tukey's range test]] most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.\n\nFollowing ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds.<ref name=\"Wilkinson\" /><ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 7.5: Comparison of Treatments; Multiple Comparison Procedures)</ref> There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.<ref>Howell (2002, Chapter 12: Multiple comparisons among treatment means)</ref><ref>Montgomery (2001, Section 3-5: Practical interpretation of results)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Effect size"}]}], "id": 24}, {"title": "Preparatory analysis", "paragraphs": "\n====The number of experimental units====\n\nIn the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.\n\nEarly experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.\n\nReporting sample size analysis is generally required in psychology. \"Provide information on sample size and the process that led to sample size decisions.\"<ref>Wilkinson (1999, p 596)</ref>  The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.\n\nBesides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting\nthe probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.<ref>Montgomery (2001, Section 3-7: Determining sample size)</ref>\n\n====Power analysis====\n[[Statistical power|Power analysis]] is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.<ref>Howell (2002, Chapter 8: Power)</ref><ref>Howell (2002, Section 11.12: Power (in ANOVA))</ref><ref>Howell (2002, Section 13.7: Power analysis for factorial experiments)</ref><ref>Moore and McCabe (2003, pp 778\u2013780)</ref>\n\n====Effect size====\n{{Main|Effect size}}\nSeveral standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable or the overall standardized difference of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines.  However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately \"meaningful\" units may be preferable for reporting purposes.<ref name=\"Wilkinson\">Wilkinson (1999, p 599)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Effect size"}]}], "id": 25}, {"title": "The number of experimental units", "paragraphs": "\nIn the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.\n\nEarly experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.\n\nReporting sample size analysis is generally required in psychology. \"Provide information on sample size and the process that led to sample size decisions.\"<ref>Wilkinson (1999, p 596)</ref>  The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.\n\nBesides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting\nthe probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.<ref>Montgomery (2001, Section 3-7: Determining sample size)</ref>\n\n", "attributes": [], "id": 26}, {"title": "Power analysis", "paragraphs": "[[Statistical power|Power analysis]] is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.<ref>Howell (2002, Chapter 8: Power)</ref><ref>Howell (2002, Section 11.12: Power (in ANOVA))</ref><ref>Howell (2002, Section 13.7: Power analysis for factorial experiments)</ref><ref>Moore and McCabe (2003, pp 778\u2013780)</ref>\n\n", "attributes": [], "id": 27}, {"title": "Effect size", "paragraphs": "{{Main|Effect size}}\nSeveral standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable or the overall standardized difference of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines.  However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately \"meaningful\" units may be preferable for reporting purposes.<ref name=\"Wilkinson\">Wilkinson (1999, p 599)</ref>\n\n", "attributes": [{"name": "Main", "values": [{"name": "1", "value": "Effect size"}]}], "id": 28}, {"title": "Follow-up analysis", "paragraphs": "It is always appropriate to carefully consider outliers.  They have a disproportionate impact on statistical conclusions and are often the result of errors.\n\n====Model confirmation====\nIt is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm [[homoscedasticity]] and gross normality.<ref>Montgomery (2001, Section 3-4: Model adequacy checking)</ref>  Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and \nmodeled data values. Trends hint at interactions among factors or among observations.  One rule of thumb: \"If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results \nwill still be approximately correct.\"<ref>Moore and McCabe (2003, p 755, Qualifications to this rule appear in a footnote.)</ref>\n\n====Follow-up tests====\nA statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned ([[A priori and a posteriori|a priori]]) or [[Post-hoc analysis|post hoc]]. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.\n\nOften one of the \"treatments\" is none, so the treatment group can act as a control. [[Dunnett's test]] (a modification of the ''t''-test) tests whether each of the other treatment groups has the same mean as the control.<ref>Montgomery (2001, Section 3-5.8: Experiments with a single factor: The analysis of variance; Practical interpretation of results; Comparing means with a control)</ref>\n\nPost hoc tests such as [[Tukey's range test]] most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.\n\nFollowing ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds.<ref name=\"Wilkinson\" /><ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 7.5: Comparison of Treatments; Multiple Comparison Procedures)</ref> There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.<ref>Howell (2002, Chapter 12: Multiple comparisons among treatment means)</ref><ref>Montgomery (2001, Section 3-5: Practical interpretation of results)</ref>\n\n", "attributes": [], "id": 29}, {"title": "Model confirmation", "paragraphs": "It is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm [[homoscedasticity]] and gross normality.<ref>Montgomery (2001, Section 3-4: Model adequacy checking)</ref>  Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and \nmodeled data values. Trends hint at interactions among factors or among observations.  One rule of thumb: \"If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results \nwill still be approximately correct.\"<ref>Moore and McCabe (2003, p 755, Qualifications to this rule appear in a footnote.)</ref>\n\n", "attributes": [], "id": 30}, {"title": "Follow-up tests", "paragraphs": "A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned ([[A priori and a posteriori|a priori]]) or [[Post-hoc analysis|post hoc]]. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.\n\nOften one of the \"treatments\" is none, so the treatment group can act as a control. [[Dunnett's test]] (a modification of the ''t''-test) tests whether each of the other treatment groups has the same mean as the control.<ref>Montgomery (2001, Section 3-5.8: Experiments with a single factor: The analysis of variance; Practical interpretation of results; Comparing means with a control)</ref>\n\nPost hoc tests such as [[Tukey's range test]] most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.\n\nFollowing ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds.<ref name=\"Wilkinson\" /><ref>Hinkelmann and Kempthorne (2008, Volume 1, Section 7.5: Comparison of Treatments; Multiple Comparison Procedures)</ref> There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.<ref>Howell (2002, Chapter 12: Multiple comparisons among treatment means)</ref><ref>Montgomery (2001, Section 3-5: Practical interpretation of results)</ref>\n\n", "attributes": [], "id": 31}, {"title": "Study designs", "paragraphs": "There are several types of ANOVA. Many statisticians base ANOVA on the [[experimental design|design of the experiment]],<ref>Cochran & Cox (1957, p 9,\n\"[T]he general rule [is] that the way in which the experiment is conducted determines not only whether inferences can be made, but also the calculations required to make them.\")</ref> especially on the protocol that specifies the [[random assignment]] of treatments to subjects; the protocol's description of the assignment mechanism should include a specification of the structure of the treatments and of any [[blocking (statistics)|blocking]]. It is also common to apply ANOVA to observational data using an appropriate statistical model.{{Citation needed|date=May 2011}}\n\nSome popular designs use the following types of ANOVA:\n*[[One-way ANOVA]] is used to test for differences among two or more [[statistical independence|independent]] groups (means), e.g. different levels of urea application in a crop, or different levels of antibiotic action on several different bacterial species,<ref>[http://www.biomedicalstatistics.info/en/multiplegroups/one-way-anova.html One-way/single factor ANOVA. Biomedical Statistics] {{webarchive |url=https://web.archive.org/web/20141107211953/http://www.biomedicalstatistics.info/en/multiplegroups/one-way-anova.html |date=7 November 2014 }}</ref> or different levels of effect of some medicine on groups of patients. However, should these groups not be independent, and there is an order in the groups (such as mild, moderate and severe disease), or in the dose of a drug (such as 5&nbsp;mg/mL, 10&nbsp;mg/mL, 20&nbsp;mg/mL) given to the same group of patients, then a [[linear trend estimation]] should be used. Typically, however, the one-way ANOVA is used to test for differences among at least three groups, since the two-group case can be covered by a [[t-test]].<ref>{{Cite journal \n| doi = 10.1093/biomet/6.1.1 \n| title = The Probable Error of a Mean \n| journal = Biometrika \n| volume = 6 \n| pages = 1\u201325\n| year = 1908 \n| pmid =  \n| pmc = \n| url = http://dml.cz/bitstream/handle/10338.dmlcz/143545/ActaOlom_52-2013-2_12.pdf \n| hdl = 10338.dmlcz/143545 \n}}</ref> When there are only two means to compare, the [[t-test]] and the ANOVA [[F-test|''F''-test]] are equivalent; the relation between ANOVA and ''t'' is given by ''F''&nbsp;=&nbsp;''t''<sup>2</sup>.\n*[[Factorial experiment|Factorial]] ANOVA is used when the experimenter wants to study the interaction effects among the treatments.\n*[[Repeated measures]] ANOVA is used when the same subjects are used for each treatment (e.g., in a [[longitudinal study]]).\n*[[Multivariate analysis of variance]] (MANOVA) is used when there is more than one [[dependent variable|response variable]].\n\n", "attributes": [{"name": "Citation needed", "values": [{"name": "date", "value": "May 2011"}]}, {"name": "webarchive ", "values": [{"name": "url", "value": "https://web.archive.org/web/20141107211953/http://www.biomedicalstatistics.info/en/multiplegroups/one-way-anova.html"}, {"name": "date", "value": "7 November 2014"}]}, {"name": "Cite journal \n", "values": [{"name": "doi", "value": "10.1093/biomet/6.1.1"}, {"name": "title", "value": "The Probable Error of a Mean"}, {"name": "journal", "value": "Biometrika"}, {"name": "volume", "value": "6"}, {"name": "pages", "value": "1\u201325"}, {"name": "year", "value": "1908"}, {"name": "url", "value": "http://dml.cz/bitstream/handle/10338.dmlcz/143545/ActaOlom_52-2013-2_12.pdf"}, {"name": "hdl", "value": "10338.dmlcz/143545"}]}], "id": 32}, {"title": "Cautions", "paragraphs": "Balanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; Unbalanced \nexperiments offer more complexity.  For single-factor (one-way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power.<ref>Montgomery (2001, Section 3-3.4: Unbalanced data)</ref>  For more complex designs the lack of balance leads to further complications. \"The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case. This means that the usual analysis of variance techniques do not apply.  \nConsequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs.\"<ref>Montgomery (2001, Section 14-2: Unbalanced data in factorial design)</ref>  In the general case, \"The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and ''F''-ratios will depend on the order in which the sources of variation \nare considered.\"<ref name=\"Gelman\" />  The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data.  More complex techniques use regression.\n\nANOVA is (in part) a test of statistical significance.  The American Psychological Association (and many other organisations) holds the view that simply reporting statistical significance is insufficient and that reporting confidence bounds is preferred.<ref name=\"Wilkinson\" />\n\nWhile ANOVA is conservative (in maintaining a significance level) against [[multiple comparisons]] in one dimension, it is not conservative against comparisons in multiple dimensions.<ref>Wilkinson (1999, p 600)</ref>\n\nA common mistake is to use an ANOVA (or Krukal-Wallis) for analysis of ordered groups, e.g. in time sequence (changes over months), in disease severity (mild, moderate, severe), or in distance from a set point (10 km, 25 km, 50 km). Data in three or more ordered groups that are defined by the researcher should be analysed by [[Linear Trend Estimation]].\n\n", "attributes": [], "id": 33}, {"title": "Generalizations", "paragraphs": "ANOVA is considered to be a special case of [[linear regression]]<ref>Gelman (2005, p.1) (with qualification in the later text)</ref><ref>Montgomery (2001, Section 3.9: The Regression Approach to the Analysis of Variance)</ref> which in turn is a special case of the [[general linear model]].<ref>Howell (2002, p 604)</ref> All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized.\n\nThe [[Kruskal\u2013Wallis test]] and the [[Friedman test]] are [[nonparametric]] tests, which  do not rely on an assumption of normality.<ref>Howell (2002, Chapter 18: Resampling and nonparametric approaches to data)</ref><ref>Montgomery (2001, Section 3-10: Nonparametric methods in the analysis of variance)</ref>\n\n===Connection to linear regression===\nBelow we make clear the connection between multi-way ANOVA and linear regression.\n\nLinearly re-order the data so that <math>k^\\text{th}</math> observation is associated with a response <math>y_k</math> and factors <math>Z_{k,b}</math> where <math>b \\in \\{1,2,\\ldots,B\\}</math> denotes the different factors and <math>B</math> is the total number of factors. In one-way ANOVA <math>B=1</math> and in two-way ANOVA <math>B=2</math>. Furthermore, we assume the <math>b^{th}</math> factor has <math>I_b</math> levels, namely <math>\\{1,2,\\ldots,I_b\\}</math>. Now, we can [[one-hot]] encode the factors into the <math> \\sum_{b=1}^B I_b</math> dimensional vector <math>v_k</math>.\n\nThe one-hot encoding function <math>g_b : \\{1,2,\\ldots,I_b\\} \\mapsto \\{0,1\\}^{I_b}</math> is defined such that the <math>i^{th}</math> entry of <math>g_b(Z_{k,b})</math> is\n<math display=\"block\">\ng_b(Z_{k,b})_i = \\begin{cases} 1 & \\text{if } i=Z_{k,b} \\\\\n 0 & \\text{otherwise} \\end{cases}\n</math>\nThe vector <math>v_k</math> is the concatenation of all of the above vectors for all <math>b</math>. Thus, <math>v_k = [g_1(Z_{k,1}), g_2(Z_{k,2}), \\ldots, g_B(Z_{k,B})]</math>. In order to obtain a fully general <math>B</math>-way interaction ANOVA we must also concatenate every additional interaction term in the vector <math>v_k</math> and then add an intercept term.  Let that vector be <math>X_k</math>.\n\nWith this notation in place, we now have the exact connection with linear regression. We simply regress response <math>y_k</math> against the vector <math>X_k</math>. However, there is a concern about identifiability. In order to overcome such issues we assume that the sum of the parameters within each set of interactions is equal to zero. From here, one can use ''F''-statistics or other methods to determine the relevance of the individual factors.\n\n====Example====\nWe can consider the 2-way interaction example where we assume that the first factor has 2 levels and the second factor has 3 levels.\n\nDefine <math>a_i = 1</math> if <math>Z_{k,1}=i</math> and <math>b_i = 1</math> if <math>Z_{k,2}=i</math>, i.e. <math>a</math> is the one-hot encoding of the first factor and <math>b</math> is the one-hot encoding of the second factor.\n\nWith that,\n<math display=\"block\">\nX_k = [a_1,a_2,b_1,b_2,b_3,a_1 \\times b_1,a_1 \\times b_2, a_1 \\times b_3, a_2 \\times b_1, a_2 \\times b_2, a_2 \\times b_3,1]\n</math>\nwhere the last term is an intercept term. For a more concrete example suppose that\n<math display=\"block\">\n\\begin{align}\nZ_{k,1} & = 2 \\\\\nZ_{k,2} & = 1\n\\end{align}\n</math>\nThen,\n<math display=\"block\">\nX_k = [0,1,1,0,0,0,0,0,1,0,0,1]\n</math>\n\n", "attributes": [], "id": 34}, {"title": "Connection to linear regression", "paragraphs": "Below we make clear the connection between multi-way ANOVA and linear regression.\n\nLinearly re-order the data so that <math>k^\\text{th}</math> observation is associated with a response <math>y_k</math> and factors <math>Z_{k,b}</math> where <math>b \\in \\{1,2,\\ldots,B\\}</math> denotes the different factors and <math>B</math> is the total number of factors. In one-way ANOVA <math>B=1</math> and in two-way ANOVA <math>B=2</math>. Furthermore, we assume the <math>b^{th}</math> factor has <math>I_b</math> levels, namely <math>\\{1,2,\\ldots,I_b\\}</math>. Now, we can [[one-hot]] encode the factors into the <math> \\sum_{b=1}^B I_b</math> dimensional vector <math>v_k</math>.\n\nThe one-hot encoding function <math>g_b : \\{1,2,\\ldots,I_b\\} \\mapsto \\{0,1\\}^{I_b}</math> is defined such that the <math>i^{th}</math> entry of <math>g_b(Z_{k,b})</math> is\n<math display=\"block\">\ng_b(Z_{k,b})_i = \\begin{cases} 1 & \\text{if } i=Z_{k,b} \\\\\n 0 & \\text{otherwise} \\end{cases}\n</math>\nThe vector <math>v_k</math> is the concatenation of all of the above vectors for all <math>b</math>. Thus, <math>v_k = [g_1(Z_{k,1}), g_2(Z_{k,2}), \\ldots, g_B(Z_{k,B})]</math>. In order to obtain a fully general <math>B</math>-way interaction ANOVA we must also concatenate every additional interaction term in the vector <math>v_k</math> and then add an intercept term.  Let that vector be <math>X_k</math>.\n\nWith this notation in place, we now have the exact connection with linear regression. We simply regress response <math>y_k</math> against the vector <math>X_k</math>. However, there is a concern about identifiability. In order to overcome such issues we assume that the sum of the parameters within each set of interactions is equal to zero. From here, one can use ''F''-statistics or other methods to determine the relevance of the individual factors.\n\n====Example====\nWe can consider the 2-way interaction example where we assume that the first factor has 2 levels and the second factor has 3 levels.\n\nDefine <math>a_i = 1</math> if <math>Z_{k,1}=i</math> and <math>b_i = 1</math> if <math>Z_{k,2}=i</math>, i.e. <math>a</math> is the one-hot encoding of the first factor and <math>b</math> is the one-hot encoding of the second factor.\n\nWith that,\n<math display=\"block\">\nX_k = [a_1,a_2,b_1,b_2,b_3,a_1 \\times b_1,a_1 \\times b_2, a_1 \\times b_3, a_2 \\times b_1, a_2 \\times b_2, a_2 \\times b_3,1]\n</math>\nwhere the last term is an intercept term. For a more concrete example suppose that\n<math display=\"block\">\n\\begin{align}\nZ_{k,1} & = 2 \\\\\nZ_{k,2} & = 1\n\\end{align}\n</math>\nThen,\n<math display=\"block\">\nX_k = [0,1,1,0,0,0,0,0,1,0,0,1]\n</math>\n\n", "attributes": [], "id": 35}, {"title": "Example", "paragraphs": "We can consider the 2-way interaction example where we assume that the first factor has 2 levels and the second factor has 3 levels.\n\nDefine <math>a_i = 1</math> if <math>Z_{k,1}=i</math> and <math>b_i = 1</math> if <math>Z_{k,2}=i</math>, i.e. <math>a</math> is the one-hot encoding of the first factor and <math>b</math> is the one-hot encoding of the second factor.\n\nWith that,\n<math display=\"block\">\nX_k = [a_1,a_2,b_1,b_2,b_3,a_1 \\times b_1,a_1 \\times b_2, a_1 \\times b_3, a_2 \\times b_1, a_2 \\times b_2, a_2 \\times b_3,1]\n</math>\nwhere the last term is an intercept term. For a more concrete example suppose that\n<math display=\"block\">\n\\begin{align}\nZ_{k,1} & = 2 \\\\\nZ_{k,2} & = 1\n\\end{align}\n</math>\nThen,\n<math display=\"block\">\nX_k = [0,1,1,0,0,0,0,0,1,0,0,1]\n</math>\n\n", "attributes": [], "id": 36}, {"title": "See also", "paragraphs": "{{Commons category|Analysis of variance}}\n\n*[[One-way analysis of variance]] ('''one-way ANOVA''')\n*[[Two-way analysis of variance]] ('''two-way ANOVA''')\n*[[ANOVA on ranks]]\n*[[ANOVA-simultaneous component analysis]]\n*[[Analysis of covariance]] ('''ANCOVA''')\n*[[Multivariate analysis of variance]] ('''MANOVA''')\n*[[Multivariate analysis of covariance]] ('''MANCOVA''')\n*[[Analysis of molecular variance]] (AMOVA)\n*[[Analysis of rhythmic variance]] (ANORVA)\n*[[Explained variation]]\n*[[Mixed-design analysis of variance]]\n*[[Permutational analysis of variance]]\n*[[Repeated measures design#Repeated measures ANOVA|Repeated measures ANOVA]]\n*[[Variance decomposition]]\n*[[Linear trend estimation]]\n\n", "attributes": [{"name": "Commons category", "values": [{"name": "1", "value": "Analysis of variance"}]}], "id": 37}, {"title": "Footnotes", "paragraphs": "{{reflist|group=\"nb\"}}\n\n", "attributes": [{"name": "reflist", "values": [{"name": "group", "value": "\"nb\""}]}], "id": 38}, {"title": "Notes", "paragraphs": "{{reflist|30em}}\n\n", "attributes": [{"name": "reflist", "values": [{"name": "1", "value": "30em"}]}], "id": 39}, {"title": "References", "paragraphs": "* {{cite journal|doi=10.2307/2984159|title=The Validity of Comparative Experiments|authorlink=Francis J. Anscombe|first=F. J.|last=Anscombe|journal=Journal of the Royal Statistical Society. Series A (General)|volume=111|issue=3|year=1948|pages=181\u2013211|jstor=2984159|mr=30181}}\n* {{cite book |last=Bailey|first=R. A.|authorlink=Rosemary A. Bailey|title=Design of Comparative Experiments|publisher=Cambridge University Press|year=2008 |isbn=978-0-521-68357-9|url=http://www.maths.qmul.ac.uk/~rab/DOEbook}} Pre-publication chapters are available on-line.\n* {{cite book | last = Belle | first = Gerald van\n| title = Statistical rules of thumb | publisher = Wiley \n| location = Hoboken, N.J | year = 2008 | edition = 2nd \n| isbn = 978-0-470-14448-0 }}\n* {{cite book | last1 = Cochran | first1 = William G.\n| last2 = Cox | first2 = Gertrude M. \n| title = Experimental designs | publisher = Wiley | location = New York \n| year = 1992 | isbn = 978-0-471-54567-5 | edition = 2nd }}\n* Cohen, Jacob (1988). ''Statistical power analysis for the behavior sciences'' (2nd ed.). Routledge {{ISBN|978-0-8058-0283-2}}\n* {{Cite journal | doi = 10.1037/0033-2909.112.1.155 | author = Cohen, Jacob | year = 1992 | title = Statistics a power primer | url = | journal = Psychological Bulletin | volume = 112 | issue = 1| pages = 155\u2013159 | pmid=19565683}}\n*[[David R. Cox|Cox, David R.]] (1958). ''Planning of experiments''.  Reprinted as {{ISBN|978-0-471-57429-3}}\n*{{cite book | last = Cox | first = D. R.\n| title = Principles of statistical inference \n| publisher = Cambridge University Press \n| location = Cambridge New York | year = 2006 \n| isbn = 978-0-521-68567-2 }}\n* [[David A. Freedman (statistician)|Freedman, David A.]](2005). ''Statistical Models: Theory and Practice'', Cambridge University Press.  {{ISBN|978-0-521-67105-7}}\n* {{Cite journal\n| last1 = Gelman | first1 = Andrew \n| doi = 10.1214/009053604000001048 \n| title = Analysis of variance? Why it is more important than ever \n| journal = The Annals of Statistics \n| volume = 33 \n| pages = 1\u201353 \n| year = 2005  \n| arxiv = math/0504499 \n}}\n*{{cite book | last = Gelman | first = Andrew\n| title = The new Palgrave dictionary of economics \n| publisher = Palgrave Macmillan \n| location = Basingstoke, Hampshire New York\n| chapter=Variance, analysis of |edition=2nd \n| year = 2008 | isbn = 978-0-333-78676-5}}\n*{{cite book\n|author=Hinkelmann, Klaus |author2=Kempthorne, Oscar |last-author-amp=yes|year=2008|title=Design and Analysis of Experiments|volume=I and II|edition=Second|publisher=Wiley|isbn=978-0-470-38551-7|author2-link=Oscar Kempthorne }}\n* {{cite book | last = Howell | first = David C. | title = Statistical methods for psychology | publisher = Duxbury/Thomson Learning | location = Pacific Grove, CA | year = 2002 | edition = 5th | isbn = 978-0-534-37770-0 | url-access = registration | url = https://archive.org/details/statisticalmetho0000howe }}\n*{{cite book\n|author=Kempthorne, Oscar\n|year=1979\n|title=The Design and Analysis of Experiments\n|edition=Corrected reprint of (1952) Wiley\n|publisher=Robert E. Krieger\n|isbn=978-0-88275-105-4\n|author-link=Oscar Kempthorne\n}}\n* Lehmann, E.L. (1959) Testing Statistical Hypotheses.  John Wiley & Sons.\n* {{cite book | last = Montgomery | first = Douglas C.\n| title = Design and Analysis of Experiments \n| publisher =  Wiley | location = New York \n| year = 2001 | edition = 5th | isbn = 978-0-471-31649-7}}\n* Moore, David S. & McCabe, George P. (2003).  Introduction to the Practice of Statistics (4e).  W H Freeman & Co.  {{ISBN|0-7167-9657-0}}\n* Rosenbaum, Paul R. (2002). ''Observational Studies'' (2nd ed.). New York: Springer-Verlag.  {{ISBN|978-0-387-98967-9}}\n* {{cite book |title=The Analysis of Variance\n|last=Scheff\u00e9 |first=Henry |location=New York\n|publisher=Wiley |year=1959}}\n*{{cite book | last = Stigler | first = Stephen M. | title = The history of statistics : the measurement of uncertainty before 1900 | publisher = Belknap Press of Harvard University Press | location = Cambridge, Mass | year = 1986 | isbn = 978-0-674-40340-6 | url-access = registration | url = https://archive.org/details/historyofstatist00stig }}\n* {{Cite journal\n|author = Wilkinson, Leland \n|title = Statistical Methods in Psychology Journals; Guidelines and Explanations \n|journal = American Psychologist \n|volume = 5\n|issue = 8\n|pages = 594\u2013604 \n|year = 1999\n|doi = 10.1037/0003-066X.54.8.594|citeseerx = 10.1.1.120.4818}}\n\n", "attributes": [{"name": "cite journal", "values": [{"name": "doi", "value": "10.2307/2984159"}, {"name": "title", "value": "The Validity of Comparative Experiments"}, {"name": "authorlink", "value": "Francis J. Anscombe"}, {"name": "first", "value": "F. J."}, {"name": "last", "value": "Anscombe"}, {"name": "journal", "value": "Journal of the Royal Statistical Society. Series A (General)"}, {"name": "volume", "value": "111"}, {"name": "issue", "value": "3"}, {"name": "year", "value": "1948"}, {"name": "pages", "value": "181\u2013211"}, {"name": "jstor", "value": "2984159"}, {"name": "mr", "value": "30181"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Bailey"}, {"name": "first", "value": "R. A."}, {"name": "authorlink", "value": "Rosemary A. Bailey"}, {"name": "title", "value": "Design of Comparative Experiments"}, {"name": "publisher", "value": "Cambridge University Press"}, {"name": "year", "value": "2008"}, {"name": "isbn", "value": "978-0-521-68357-9"}, {"name": "url", "value": "http://www.maths.qmul.ac.uk/~rab/DOEbook"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Belle"}, {"name": "first", "value": "Gerald van"}, {"name": "title", "value": "Statistical rules of thumb"}, {"name": "publisher", "value": "Wiley"}, {"name": "location", "value": "Hoboken, N.J"}, {"name": "year", "value": "2008"}, {"name": "edition", "value": "2nd"}, {"name": "isbn", "value": "978-0-470-14448-0"}]}, {"name": "cite book ", "values": [{"name": "last1", "value": "Cochran"}, {"name": "first1", "value": "William G."}, {"name": "last2", "value": "Cox"}, {"name": "first2", "value": "Gertrude M."}, {"name": "title", "value": "Experimental designs"}, {"name": "publisher", "value": "Wiley"}, {"name": "location", "value": "New York"}, {"name": "year", "value": "1992"}, {"name": "isbn", "value": "978-0-471-54567-5"}, {"name": "edition", "value": "2nd"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-8058-0283-2"}]}, {"name": "Cite journal ", "values": [{"name": "doi", "value": "10.1037/0033-2909.112.1.155"}, {"name": "author", "value": "Cohen, Jacob"}, {"name": "year", "value": "1992"}, {"name": "title", "value": "Statistics a power primer"}, {"name": "journal", "value": "Psychological Bulletin"}, {"name": "volume", "value": "112"}, {"name": "issue", "value": "1"}, {"name": "pages", "value": "155\u2013159"}, {"name": "pmid", "value": "19565683"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-471-57429-3"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Cox"}, {"name": "first", "value": "D. R."}, {"name": "title", "value": "Principles of statistical inference"}, {"name": "publisher", "value": "Cambridge University Press"}, {"name": "location", "value": "Cambridge New York"}, {"name": "year", "value": "2006"}, {"name": "isbn", "value": "978-0-521-68567-2"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-521-67105-7"}]}, {"name": "Cite journal\n", "values": [{"name": "last1", "value": "Gelman"}, {"name": "first1", "value": "Andrew"}, {"name": "doi", "value": "10.1214/009053604000001048"}, {"name": "title", "value": "Analysis of variance? Why it is more important than ever"}, {"name": "journal", "value": "The Annals of Statistics"}, {"name": "volume", "value": "33"}, {"name": "pages", "value": "1\u201353"}, {"name": "year", "value": "2005"}, {"name": "arxiv", "value": "math/0504499"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Gelman"}, {"name": "first", "value": "Andrew"}, {"name": "title", "value": "The new Palgrave dictionary of economics"}, {"name": "publisher", "value": "Palgrave Macmillan"}, {"name": "location", "value": "Basingstoke, Hampshire New York"}, {"name": "chapter", "value": "Variance, analysis of"}, {"name": "edition", "value": "2nd"}, {"name": "year", "value": "2008"}, {"name": "isbn", "value": "978-0-333-78676-5"}]}, {"name": "cite book\n", "values": [{"name": "author", "value": "Hinkelmann, Klaus"}, {"name": "author2", "value": "Kempthorne, Oscar"}, {"name": "last-author-amp", "value": "yes"}, {"name": "year", "value": "2008"}, {"name": "title", "value": "Design and Analysis of Experiments"}, {"name": "volume", "value": "I and II"}, {"name": "edition", "value": "Second"}, {"name": "publisher", "value": "Wiley"}, {"name": "isbn", "value": "978-0-470-38551-7"}, {"name": "author2-link", "value": "Oscar Kempthorne"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Howell"}, {"name": "first", "value": "David C."}, {"name": "title", "value": "Statistical methods for psychology"}, {"name": "publisher", "value": "Duxbury/Thomson Learning"}, {"name": "location", "value": "Pacific Grove, CA"}, {"name": "year", "value": "2002"}, {"name": "edition", "value": "5th"}, {"name": "isbn", "value": "978-0-534-37770-0"}, {"name": "url-access", "value": "registration"}, {"name": "url", "value": "https://archive.org/details/statisticalmetho0000howe"}]}, {"name": "cite book\n", "values": [{"name": "author", "value": "Kempthorne, Oscar"}, {"name": "year", "value": "1979"}, {"name": "title", "value": "The Design and Analysis of Experiments"}, {"name": "edition", "value": "Corrected reprint of (1952) Wiley"}, {"name": "publisher", "value": "Robert E. Krieger"}, {"name": "isbn", "value": "978-0-88275-105-4"}, {"name": "author-link", "value": "Oscar Kempthorne"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Montgomery"}, {"name": "first", "value": "Douglas C."}, {"name": "title", "value": "Design and Analysis of Experiments"}, {"name": "publisher", "value": "Wiley"}, {"name": "location", "value": "New York"}, {"name": "year", "value": "2001"}, {"name": "edition", "value": "5th"}, {"name": "isbn", "value": "978-0-471-31649-7"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "0-7167-9657-0"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-387-98967-9"}]}, {"name": "cite book ", "values": [{"name": "title", "value": "The Analysis of Variance"}, {"name": "last", "value": "Scheff\u00e9"}, {"name": "first", "value": "Henry"}, {"name": "location", "value": "New York"}, {"name": "publisher", "value": "Wiley"}, {"name": "year", "value": "1959"}]}, {"name": "cite book ", "values": [{"name": "last", "value": "Stigler"}, {"name": "first", "value": "Stephen M."}, {"name": "title", "value": "The history of statistics : the measurement of uncertainty before 1900"}, {"name": "publisher", "value": "Belknap Press of Harvard University Press"}, {"name": "location", "value": "Cambridge, Mass"}, {"name": "year", "value": "1986"}, {"name": "isbn", "value": "978-0-674-40340-6"}, {"name": "url-access", "value": "registration"}, {"name": "url", "value": "https://archive.org/details/historyofstatist00stig"}]}, {"name": "Cite journal\n", "values": [{"name": "author", "value": "Wilkinson, Leland"}, {"name": "title", "value": "Statistical Methods in Psychology Journals; Guidelines and Explanations"}, {"name": "journal", "value": "American Psychologist"}, {"name": "volume", "value": "5"}, {"name": "issue", "value": "8"}, {"name": "pages", "value": "594\u2013604"}, {"name": "year", "value": "1999"}, {"name": "doi", "value": "10.1037/0003-066X.54.8.594"}, {"name": "citeseerx", "value": "10.1.1.120.4818"}]}], "id": 40}, {"title": "Further reading", "paragraphs": "{{further cleanup|date=November 2014}}\n* {{cite journal\n | last = Box | first = G. e. p. \n | authorlink = George E. P. Box\n | title = Non-Normality and Tests on Variances\n | journal = Biometrika\n | volume = 40\n | issue = 3/4\n | pages = 318\u2013335\n | year = 1953\n | jstor = 2333350\n | doi=10.1093/biomet/40.3-4.318\n}}\n* {{Cite journal\n| last1 = Box | first1 = G. E. P. |authorlink=George E. P. Box\n| title = Some Theorems on Quadratic Forms Applied in the Study of Analysis of Variance Problems, I. Effect of Inequality of Variance in the One-Way Classification \n| doi = 10.1214/aoms/1177728786 \n| journal = The Annals of Mathematical Statistics \n| volume = 25 \n| issue = 2 \n| page = 290 \n| year = 1954 \n| pmid =  \n| pmc = \n}}\n* {{Cite journal \n| last1 = Box | first1 = G. E. P. \n| title = Some Theorems on Quadratic Forms Applied in the Study of Analysis of Variance Problems, II. Effects of Inequality of Variance and of Correlation Between Errors in the Two-Way Classification \n| doi = 10.1214/aoms/1177728717 \n| journal = The Annals of Mathematical Statistics \n| volume = 25 \n| issue = 3 \n| page = 484 \n| year = 1954 \n| pmid =  \n| pmc = \n}}\n* {{cite book\n|author1=Cali\u0144ski, Tadeusz  |author2=Kageyama, Sanpei|title=Block designs: A Randomization approach, Volume '''I''': Analysis|series=Lecture Notes in Statistics|volume=150|publisher=Springer-Verlag|location=New York|year=2000|isbn=978-0-387-98578-7\n}}\n* {{cite book|title=Plane Answers to Complex Questions: The Theory of Linear Models|last=Christensen|first=Ronald|location=New York|publisher=Springer|year=2002| edition=Third|isbn=978-0-387-95361-8}}\n*[[David R. Cox|Cox, David R.]] & [[Nancy M. Reid|Reid, Nancy M.]] (2000). ''The theory of design of experiments''.  (Chapman & Hall/CRC).  {{ISBN|978-1-58488-195-7}}\n* {{Cite journal|doi=10.1017/S0021859600003750 |author=Fisher, Ronald |year=1918 |title=Studies in Crop Variation. I. An examination of the yield of dressed grain from Broadbalk |url=http://www.library.adelaide.edu.au/digitised/fisher/15.pdf |archive-url=https://web.archive.org/web/20010612211752/http://www.library.adelaide.edu.au/digitised/fisher/15.pdf |url-status=dead |archive-date=12 June 2001 |journal=Journal of Agricultural Science |volume=11 |issue= 2|pages=107\u2013135 |hdl=2440/15170 }}\n* [[David A. Freedman (statistician)|Freedman, David A.]]; Pisani, Robert; Purves, Roger (2007) ''Statistics'', 4th edition. W.W. Norton & Company {{ISBN|978-0-393-92972-0}}\n* {{cite book|last1=Hettmansperger|first1=T. P.|last2=McKean|first2=J. W.|title=Robust nonparametric statistical methods| edition=First|series=Kendall's Library of Statistics|volume=Volume 5|editor=Edward Arnold|location=New York|publisher=John Wiley & Sons, Inc.|year=1998|pages=xiv+467 pp|isbn=978-0-340-54937-7 |mr=1604954 }}\n* {{cite book\n|first=Marvin\n|last=Lentner\n|author2=Thomas Bishop\n|title=Experimental design and analysis\n|edition=Second\n|publisher=Valley Book Company\n|location=P.O. Box 884, Blacksburg, VA 24063\n|year=1993\n|isbn=978-0-9616255-2-8\n}}\n* Tabachnick, Barbara G. & Fidell, Linda S. (2007). ''Using Multivariate Statistics'' (5th ed.). Boston: Pearson International Edition.  {{ISBN|978-0-205-45938-4}}\n* {{cite book|last=Wichura|first=Michael J.|title=The coordinate-free approach to linear models|series=Cambridge Series in Statistical and Probabilistic Mathematics|publisher=Cambridge University Press|location=Cambridge|year=2006|pages=xiv+199|isbn=978-0-521-86842-6|mr=2283455|ref=harv}}\n* {{ cite book | last = Phadke | first = Madhav S.\n| title = Quality Engineering using Robust Design\n| publisher = Prentice Hall PTR \n| location = New Jersey | year = 1989 | isbn = 978-0-13-745167-8 }}\n\n", "attributes": [{"name": "further cleanup", "values": [{"name": "date", "value": "November 2014"}]}, {"name": "cite journal\n ", "values": [{"name": "last", "value": "Box"}, {"name": "first", "value": "G. e. p."}, {"name": "authorlink", "value": "George E. P. Box"}, {"name": "title", "value": "Non-Normality and Tests on Variances"}, {"name": "journal", "value": "Biometrika"}, {"name": "volume", "value": "40"}, {"name": "issue", "value": "3/4"}, {"name": "pages", "value": "318\u2013335"}, {"name": "year", "value": "1953"}, {"name": "jstor", "value": "2333350"}, {"name": "doi", "value": "10.1093/biomet/40.3-4.318"}]}, {"name": "Cite journal\n", "values": [{"name": "last1", "value": "Box"}, {"name": "first1", "value": "G. E. P."}, {"name": "authorlink", "value": "George E. P. Box"}, {"name": "title", "value": "Some Theorems on Quadratic Forms Applied in the Study of Analysis of Variance Problems, I. Effect of Inequality of Variance in the One-Way Classification"}, {"name": "doi", "value": "10.1214/aoms/1177728786"}, {"name": "journal", "value": "The Annals of Mathematical Statistics"}, {"name": "volume", "value": "25"}, {"name": "issue", "value": "2"}, {"name": "page", "value": "290"}, {"name": "year", "value": "1954"}]}, {"name": "Cite journal \n", "values": [{"name": "last1", "value": "Box"}, {"name": "first1", "value": "G. E. P."}, {"name": "title", "value": "Some Theorems on Quadratic Forms Applied in the Study of Analysis of Variance Problems, II. Effects of Inequality of Variance and of Correlation Between Errors in the Two-Way Classification"}, {"name": "doi", "value": "10.1214/aoms/1177728717"}, {"name": "journal", "value": "The Annals of Mathematical Statistics"}, {"name": "volume", "value": "25"}, {"name": "issue", "value": "3"}, {"name": "page", "value": "484"}, {"name": "year", "value": "1954"}]}, {"name": "cite book\n", "values": [{"name": "author1", "value": "Cali\u0144ski, Tadeusz"}, {"name": "author2", "value": "Kageyama, Sanpei"}, {"name": "title", "value": "Block designs: A Randomization approach, Volume '''I''': Analysis"}, {"name": "series", "value": "Lecture Notes in Statistics"}, {"name": "volume", "value": "150"}, {"name": "publisher", "value": "Springer-Verlag"}, {"name": "location", "value": "New York"}, {"name": "year", "value": "2000"}, {"name": "isbn", "value": "978-0-387-98578-7"}]}, {"name": "cite book", "values": [{"name": "title", "value": "Plane Answers to Complex Questions: The Theory of Linear Models"}, {"name": "last", "value": "Christensen"}, {"name": "first", "value": "Ronald"}, {"name": "location", "value": "New York"}, {"name": "publisher", "value": "Springer"}, {"name": "year", "value": "2002"}, {"name": "edition", "value": "Third"}, {"name": "isbn", "value": "978-0-387-95361-8"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-1-58488-195-7"}]}, {"name": "Cite journal", "values": [{"name": "doi", "value": "10.1017/S0021859600003750"}, {"name": "author", "value": "Fisher, Ronald"}, {"name": "year", "value": "1918"}, {"name": "title", "value": "Studies in Crop Variation. I. An examination of the yield of dressed grain from Broadbalk"}, {"name": "url", "value": "http://www.library.adelaide.edu.au/digitised/fisher/15.pdf"}, {"name": "archive-url", "value": "https://web.archive.org/web/20010612211752/http://www.library.adelaide.edu.au/digitised/fisher/15.pdf"}, {"name": "url-status", "value": "dead"}, {"name": "archive-date", "value": "12 June 2001"}, {"name": "journal", "value": "Journal of Agricultural Science"}, {"name": "volume", "value": "11"}, {"name": "issue", "value": "2"}, {"name": "pages", "value": "107\u2013135"}, {"name": "hdl", "value": "2440/15170"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-393-92972-0"}]}, {"name": "cite book", "values": [{"name": "last1", "value": "Hettmansperger"}, {"name": "first1", "value": "T. P."}, {"name": "last2", "value": "McKean"}, {"name": "first2", "value": "J. W."}, {"name": "title", "value": "Robust nonparametric statistical methods"}, {"name": "edition", "value": "First"}, {"name": "series", "value": "Kendall's Library of Statistics"}, {"name": "volume", "value": "Volume 5"}, {"name": "editor", "value": "Edward Arnold"}, {"name": "location", "value": "New York"}, {"name": "publisher", "value": "John Wiley & Sons, Inc."}, {"name": "year", "value": "1998"}, {"name": "pages", "value": "xiv+467 pp"}, {"name": "isbn", "value": "978-0-340-54937-7"}, {"name": "mr", "value": "1604954"}]}, {"name": "cite book\n", "values": [{"name": "first", "value": "Marvin"}, {"name": "last", "value": "Lentner"}, {"name": "author2", "value": "Thomas Bishop"}, {"name": "title", "value": "Experimental design and analysis"}, {"name": "edition", "value": "Second"}, {"name": "publisher", "value": "Valley Book Company"}, {"name": "location", "value": "P.O. Box 884, Blacksburg, VA 24063"}, {"name": "year", "value": "1993"}, {"name": "isbn", "value": "978-0-9616255-2-8"}]}, {"name": "ISBN", "values": [{"name": "1", "value": "978-0-205-45938-4"}]}, {"name": "cite book", "values": [{"name": "last", "value": "Wichura"}, {"name": "first", "value": "Michael J."}, {"name": "title", "value": "The coordinate-free approach to linear models"}, {"name": "series", "value": "Cambridge Series in Statistical and Probabilistic Mathematics"}, {"name": "publisher", "value": "Cambridge University Press"}, {"name": "location", "value": "Cambridge"}, {"name": "year", "value": "2006"}, {"name": "pages", "value": "xiv+199"}, {"name": "isbn", "value": "978-0-521-86842-6"}, {"name": "mr", "value": "2283455"}, {"name": "ref", "value": "harv"}]}, {"name": " cite book ", "values": [{"name": "last", "value": "Phadke"}, {"name": "first", "value": "Madhav S."}, {"name": "title", "value": "Quality Engineering using Robust Design"}, {"name": "publisher", "value": "Prentice Hall PTR"}, {"name": "location", "value": "New Jersey"}, {"name": "year", "value": "1989"}, {"name": "isbn", "value": "978-0-13-745167-8"}]}], "id": 41}, {"title": "External links", "paragraphs": "{{wikiversity}}\n* [[SOCR]] [http://wiki.stat.ucla.edu/socr/index.php/AP_Statistics_Curriculum_2007_ANOVA_1Way ANOVA Activity] and [http://www.socr.ucla.edu/htmls/ana/ANOVA1Way_Analysis.html interactive applet].\n* [http://www.southampton.ac.uk/~cpd/anovas/datasets/index.htm  Examples of all ANOVA and ANCOVA models with up to three treatment factors, including randomized block, split plot, repeated measures, and Latin squares, and their analysis in R] (University of Southampton)\n* NIST/SEMATECH e-Handbook of Statistical Methods, [http://www.itl.nist.gov/div898/handbook/prc/section4/prc43.htm section 7.4.3: \"Are the means equal?\"]\n*[https://web.archive.org/web/20150405053021/http://biostat.katerynakon.in.ua/en/multiplegroups/anova.html Analysis of variance: Introduction]\n*[https://getcalc.com/statistics-anova-calculator.htm One Way & Two Way ANOVA Calculator]\n\n{{Portal bar|Mathematics}}\n{{Statistics|correlation|state=collapsed}}\n{{Experimental design|state=collapsed}}\n{{Least squares and regression analysis}}\n{{Public health}}\n\n{{DEFAULTSORT:Analysis Of Variance}}\n[[Category:Analysis of variance| ]]\n[[Category:Design of experiments]]\n[[Category:Statistical tests]]\n[[Category:Parametric statistics]]", "attributes": [{"name": "Portal bar", "values": [{"name": "1", "value": "Mathematics"}]}, {"name": "Statistics", "values": [{"name": "1", "value": "correlation"}, {"name": "state", "value": "collapsed"}]}, {"name": "Experimental design", "values": [{"name": "state", "value": "collapsed"}]}], "id": 42}]}